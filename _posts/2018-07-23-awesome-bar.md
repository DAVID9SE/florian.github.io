---
layout: post
title:  "Federated Learning for Firefox"
date:   2018-07-19 13:59:59
description: "Distributed machine learning for the Firefox URL bar"
categories: machine-learning, federated-learning
---

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

### Search in the Firefox URL bar

The Firefox URL bar shows suggestions when typing a search query.
A part of them is directly provided by a search engine.
The others are generated by Firefox, for example based on the user's history, bookmark or other open tabs.
We tried to optimize the history and bookmark suggestions with this project.

Searching for history and bookmark entries in the Firefox URL bar is a two-step process:

1. The search query is matched against the browser history and bookmarks. Matching is a binary decision. Pages either match the query or do not
2. The set of matched links is ranked based on the user's history

Our project purely tries to optimize the ranking part of this process.
Future work could tackle the problem directly from the query matching.

### Learning to Rank

Before diving into the current implementation, it is worth taking a step back to understand how ranking in machine learning works.
This makes it easier to see how the current algorithm fits into a machine learning system.
Fundamentally, there are three different approaches for learning a ranking algorithm:

1. *Pointwise ranking*: Each item is given separately to the model, which assigns a score to the item. The ranking is then determined by sorting all items using their respective scores. Essentially, this is a special type of a regression model since we are assigning a real number to every input
2. *Pairwise ranking*: The model learns to compare pairs of items. Its task is to decide which of the two items should be ranked higher. Intuitively, this method can be motivated by the fact that the learned comparison function could then be used by various sorting algorithms. In this approach, we treat the problem as a classification task since the model can only have two possible outputs
3. *Listwise ranking*: Instead of only working with individual items in each step, these methods try to operate on the entire list. The motivation behind this idea is that the evaluation metric can be directly optimized. In practice, this turns out to be fairly difficult because many evaluation metrics are not differentiable and the models need to work in a much higher dimension. Another difficulty is that the list could have an arbitrary length

All these approaches have different advantages and disadvantages.
The existing ranking algorithm in Firefox is very similar to a pointwise ranking approach.
Since this algorithm should be optimized using machine learning techniques, it is a reasonable decision to keep working with pointwise techniques in this project.

### Frecency

The ranking of possible suggestions in the Firefox URL bar is performed using *frecency*, an algorithm that weights how *frequently* and *recently* a site was visited.
To do this, a frecency score is assigned to each history entry and bookmark entry.
After computing the score, it is cached.
When searching, the matched results are then sorted using this score.
This section introduces the existing frecency algorithm, while the next one explains its shortcomings.

Frecency not only takes frecency and recency into account but also how the page was visited and if it is bookmarked.
It does this by looking at the last visits to the respective site.
The value \\(\operatorname{visit}(v)\\) of one single visit \\(v\\) is then defined by how recent that visit was, scaled by the type of visit:

$$
\operatorname{visit}(v) = \operatorname{recency}(v) * \operatorname{type}(v)
$$

Frecency scores have to be cached in order to allow an efficient ranking while the user is typing.
This means that the recency aspect has to be modelled using time buckets.
Otherwise the score would change all the time and caching would not work.
In the current Firefox implementation, there are five time buckets.
With this approach, the recency score only changes when a visit changes time buckets:

$$
\operatorname{recency}(v) = \begin{cases}
	100 & \text{if visited in the past 4 days} \\
	 70 & \text{if visited in the past 14 days} \\
	 50 & \text{if visited in the past 31 days} \\
	 30 & \text{if visited in the past 90 days} \\
	 10 & \text{otherwise}
\end{cases}
$$

Sites can be visited in many different ways.
If the user typed the entire link themselves or if it was a bookmarked link, we want to weight that differently to visiting a page by clicking a link.
Other visit types, like some types of redirects, should not be worth any score at all.
We implement this by scaling the recency score with a type score:

$$
\operatorname{type}(v) = \begin{cases}
	1.2 & \text{if normal visit} \\
	2   & \text{if link was typed out} \\
	1.4 & \text{if link is bookmarked} \\
	0   & \text{otherwise}
\end{cases}
$$

Now that we can assign a score to every visit, we could determine the full points for a page by summing up the scores of each visit to that page.
This approach has several disadvantages.
For one, it would scale badly because some pages are visited a lot.
Additionally, user preferences change over time and we might want to decrease the points in some situations.

Instead, we compute the average score of the last 10 visits.
This score is then scaled by the total number of visits.
The full frecency score can now be efficiently computed and changes in user behaviour can be reflected fairly quickly.
Let \\(S_x\\) be the set of all visits to page \\(x\\) and \\(T_x\\) the set of the last 10 of these.
The full frecency score is then given by:

$$
\operatorname{frecency}(x) = \frac{|S_x|}{|T_x|} * \sum\limits_{v \in T_x} \operatorname{visit}(v)
$$

Note that this is a simplified version of the algorithm.
There is some additional logic for special cases, like typing out bookmarks or different kinds of redirects.
The description here only shows the essence of the algorithm in a mathematical form.

### Optimizing Frecency

While frecency has been working pretty well in Firefox, the weights in the algorithm were not decided on in a data-driven way.
Essentially, they are similar to [magic numbers](https://en.wikipedia.org/wiki/Magic_number_(programming)).
They were chosen because they seemed reasonable but there is nothing that explains why these numbers should be the ones being used.
Maybe different time buckets or different weights would lead to much better results.

Our project replaces the constants by variables that we optimize for.
This is done for all numbers in the previous section, except for two:
- number of considered visits (10): If this number increases too much, it would hurt performance. The current value represents a good trade-off between performance and using a sufficient amount of information
- number of time buckets (5): Optimizing this would be harder and with the current implementation there was also no easy way of changing this value

There are 22 remaining weights in the full algorithm that are optimized using our project.
By doing this, more optimal values can be found, or it can at least be confirmed that the current ones were already chosen very well.
It is also a safe way of experimenting with the Firefox URL bar:
We start our optimization process from the current set of values and then try to improve them from there.

The optimization process is based on the users' interaction with the URL bar.
They are shown a set of suggestions that were ranked using our model.
If they do not choose the top one, our model can use this feedback as a signal that it needs to change the weights to improve the rank of the selected item.
Even if the top item was chosen, we can teach our model to be more confident in this decision.

### SVM Ranking

For describing this goal formally, we need to have a loss function that evaluates how well our model did.
To this end, we use an adapted form of the SVM loss.
Essentially, we are given a set of items with their assigned score and the index of the selected item.
The optimization goal is that the selected item should have the highest score.

But even if that was the case, our model might not have been to sure in that decision.
One example for this is the selected item having a score of 100 and the second item having a score of 99.9.
The model made the correct prediction, but only barely so.
To make sure it does a good job in similar cases, we need to give a signal to the model that it can still improve.

If the URL bar displayed the suggestions for pages \\(x_1, \dots, x_n\\) in that order and suggestion \\(x_i\\) was chosen, then the SVM loss for ranking is given by:

$$
\operatorname{loss} = \sum\limits_{j \neq i} \max(0, f(x_j) + \Delta - f(x_i))
$$

We iterate over all suggestions that were not chosen and check that their score was smaller than the one of the selected page by at least a margin of \\(\Delta\\).
If not, an error is added.
The full loss should be minimized.
The margin \\(\Delta\\) is a hyperparameter that needs to be decided on before the optimization process starts.

### Computing Gradients

Every time a user performes a history or bookmark search in the URL bar, we compute the SVM loss on that search.
To compute an update, we then try to move the weights a little bit into a direction where this loss is minimized.
The update corresponds to the gradient of the SVM loss with respect to the weights in the frecency algorithm that we optimize for.

Gradients can be computed elegantly using [computational graphs](http://colah.github.io/posts/2015-08-Backprop/).
By using machine learning libraries, we first construct the function we want to compute.
Afterwards, we can make use of automatic differentiation techniques to obtain the gradient.
Our initial prototyping was based on this idea.
The major advantage is that it is very easy to change the model architecture.

The frecency implementation, however, is written in C++, while the client-side part of this experiment works using JavaScript.
This made it hard to add a computational graph to the model that was shipped to users.
Reimplementing the full algorithm in JavaScript seemed like a bad idea.
Performance-wise there would be a huge penalty and it is hard to reconstruct the way the current implementation handles all the edge cases.

Instead, we decided to use a simple [finite-difference](https://en.wikipedia.org/wiki/Finite_difference) technique.
If we want to compute the gradient of a function \\(g\\) at the point \\(x\\), we check the difference of values close to that point:

$$
g'(x) \approx \frac{g(x + \epsilon) - g(x - \epsilon)}{2 * \epsilon}
$$

This formula is very close to the definition of derivatives.
To compute the gradient of a multivariate function, this process is then performed by iterating through all dimensions independently.
In each dimension, the value is changed by \\(\epsilon\\) in the two directions, while all other values stay constant.
The resulting vector is our gradient estimate.

This method is both easy to understand and implement.
For large models there is a performance penalty since we need to evaluate \\(g\\) two times for every dimension.
In \\(n\\) dimensions, there are \\(\mathcal{O}(n)\\) forward passes as opposed to \\(\mathcal{O}(1)\\) for computational graphs.
But since we only work in 22 dimensions here, this is not a major problem.

It also allows us to essentially treat frecency as a black box.
Our model does not need to know about all edge cases.
It is sufficient for the model to see how different decisions affect the output.

### Data Pipeline

The addon we built observes how users are interacting with the URL bar and retrieves all necessary information to compute the gradient.
That update together with some statistics about how well the model is doing are then sent to a Mozilla server.
This works by using the Telemetry system, which has several advantages.
It is a well-designed system with clear rules about what can be collected.
There is a lot of infrastructure around using it and dealing with the data on the server.

All messages sent by clients are stored in a [Parquet](https://parquet.apache.org/) data store.
Every 30 minutes, a Spark jobs reads the new updates and averages them.
The average update is then given to an optimizer and applied to the update.
The resulting model is published and fetched by clients.

### Updating the Model

One central problem with applying the update to the model is choosing the hyperparameters of the optimizer.
Since we did not collect any data, it is hard to tune the optimizer beforehand.
Even values like the learning rate are hard to set since we have no information about the gradient magnitude.
Trying out many different learning rates in production would take time and could lead to a bad user experience.
Directly collecting some data goes against the goal of doing machine learning without collecting data.

We tackled this problem in two ways.
First of all, we created simulations that use a made-up dataset that should be similar to the one we expect to see in production.
This allowed experimenting with different optimizers and helped with making early design decisions.
It also made it possible to quickly iterate on ideas to see if they could work.

The second way of dealing with the fact that it is hard to set hyperparameters was using the [RProp](https://florian.github.io/rprop/) optimizer.
This optimizer has major advantages in our case:
- It completely ignores the gradient magnitude and only takes into account the sign of the gradient. This means it will work with any sort of gradient we could see in production. We do not have to worry about properly scaling it
- It automatically adapts internal learning rates based on how well they work. So even if the initial values are off, they will move to decent ones in a few iterations
- The updates produced by RProp are very interpretable. In our case, we make sure they are 3 at most, so that frecency scores only change slowly

After RProp produces an update, we still apply several constraints to it.
- Weights have to be nonnegative. This means visiting a site can not directly have a negative effect
- The weights for the time buckets have to be in order. A recent visit has to be more valuable than a visit 90 days ago

These essentially act as safe-guards to make sure that user experience does not degrade too much if the optimization process fails.

### Study

Users in the experiment were split into three groups:

- *treatment*: The full study was shipped to these users. They compute updates, send them to the server and start using a new model every 30 minutes
- *control*: This group is solely observatory. No behaviour in the URL bar actually changes. We are just collecting statistics for comparison to treatment
- *control-no-decay*: Firefox decays frecency scores over time. Our treatment group loses this effect because we are recomputing scores every 30 minutes. To check if the decay is actually useful, this group has no decay effect but the same original algorithm otherwise

The study was shipped to 500,000 people.
60% of these were in the treatment group, while the other 40% were split among two control groups.

### Metrics

To evaluate how well our trained model works, we had three success criteria:
1. Do not significantly decrease the quality of the existing Firefox URL bar
2. Successfully train a model using Federated Learning
3. Stretch goal: Improve the Firefox URL bar

Actually improving the quality of the ranking for users was only a stretch goal.
The primary goal of the study was to see if it is possible to make the distributed optimization process work.
Essentially this meant consistently decreasing the loss of the model.
At the same time, the quality of the URL bar should not decrease.
The reason for distinguish between these is that our optimization goal could have been misaligned.
It is possible to minimize some loss function without actually improving the experience for the user.

To measure the quality of history and bookmark suggestions in the URL bar, we used two metrics:
- The rank of the suggestion that was selected: The item that is selected should be as far on top as possible
- Number of characters typed before selecting a result: Users should have to type few characters to find what they are looking for

### Power Analysis

### Analyzing the Results

### Future Work

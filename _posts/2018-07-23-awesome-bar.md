---
layout: post
title:  "Federated Learning for Firefox"
date:   2018-07-19 13:59:59
description: "Distributed machine learning for the Firefox URL bar"
categories: machine-learning, federated-learning
---

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

Large parts of machine learning nowadays are based on collecting a lot of data.
This data is then put on a powerful server where the training is performed.
If the data is considered private by people, using machine learning this way is not possible or a bad idea.
The entirely opposite approach, a completely decentralized system where individual models are trained for each user locally, generally works badly.
Individuals for themselves often do not have enough data to fit good models.

*Federated Learning* is a new subarea of machine learning where the training process is distributed.
The parts of the algorithm that touch the users' data are locally executed on their machines.
Instead of sharing data, clients then only send back abstract model improvements, for example weight updates, to the server.
This way, a high-quality model can be trained collaboratively without needing to collect data.
The [Federated Learning blog post]({{ site.baseurl }}{% link _posts/2018-05-09-federated-learning.md %}) on this site gives a much broader overview over this area.

Over the past three months, we implemented a Federated Learning system for Firefox.
The aim of this project was to improve part of the suggestions displayed in the Firefox URL bar, though in theory the system is flexible enough to be used in many other situations.
Collecting search queries and browsing histories is extremely bad for privacy, so using Federated Learning to still train a model on this data makes a lot of sense.
Initially, we experimented with [language models](https://en.wikipedia.org/wiki/Language_model) but then reduced the scope of the project to make it possible to implement everything in three months.

### Search in the Firefox URL bar

The Firefox URL bar shows suggestions when typing a search query.
A part of them is directly provided by a search engine.
The others are generated by Firefox itself, for example based on the user's history, bookmarks or open tabs.
We tried to optimize the history and bookmark suggestions using our project.

![Three functions with the same optima but vastly different gradients](/assets/posts/awesome-bar/bar.png)

Searching for history and bookmark entries in the Firefox URL bar is a two-step process:

1. The search query is matched against the browser history and bookmarks. Matching is a binary decision. Pages either match the query or do not
2. The set of matched links is ranked based on the user's history

Our project purely tries to optimize the ranking part of this process.
Future work could tackle the problem directly from the query matching.

### Learning to Rank

Before diving into the current implementation, it is worth taking a step back to understand how ranking in machine learning works.
This makes it easier to see how the current algorithm fits into a machine learning system.
Fundamentally, there are three different approaches for learning a ranking algorithm:

1. *Pointwise ranking*: Each item is given separately to the model, which assigns a score to the item. The ranking is then determined by sorting all items using their respective scores. Essentially, this is a special type of a regression model since we are assigning a real number to every input
2. *Pairwise ranking*: The model learns to compare pairs of items. Its task is to decide which of the two items should be ranked higher. Intuitively, this method can be motivated by the fact that the learned comparison function could then be used by various sorting algorithms. In this approach, we treat the problem as a classification task since the model can only have two possible outputs
3. *Listwise ranking*: Instead of only working with individual items in each step, these methods try to operate on the entire list. The motivation behind this idea is that the evaluation metric can be directly optimized. In practice, this turns out to be fairly difficult because many evaluation metrics are not differentiable and the models need to work with many more inputs. Another difficulty is that the list could have an arbitrary length

All these approaches have different advantages and disadvantages.
The existing ranking algorithm in Firefox is very similar to a pointwise ranking approach.
Since this algorithm should be optimized using machine learning techniques, this gives us a clear set of techniques that could be useful in this project.

### Frecency

The ranking of possible suggestions in the Firefox URL bar is performed using *frecency*, an algorithm that weights how *frequently* and *recently* a site was visited.
To do this, a frecency score is assigned to each history entry and bookmark entry.
After computing the score, it is cached.
When searching, the matched results are then sorted using this score.
This section introduces the existing frecency algorithm, while the next one explains how we wanted to improve it.

Frecency not only takes frecency and recency into account but also how the page was visited and if it is bookmarked.
It does this by looking at the last visits to the respective site.
The value \\(\operatorname{visit}(v)\\) of one single visit \\(v\\) is then defined by how recent that visit was, scaled by the type of visit:

$$
\operatorname{visit}(v) = \operatorname{recency}(v) * \operatorname{type}(v)
$$

Frecency scores have to be cached in order to allow an efficient ranking while the user is typing.
This means that the recency aspect has to be modelled using time buckets.
Otherwise the score would change all the time and caching would not work.
In the current Firefox implementation, there are five time buckets.
With this approach, the recency score only changes when a visit changes time buckets:

$$
\operatorname{recency}(v) = \begin{cases}
	100 & \text{if visited in the past 4 days} \\
	 70 & \text{if visited in the past 14 days} \\
	 50 & \text{if visited in the past 31 days} \\
	 30 & \text{if visited in the past 90 days} \\
	 10 & \text{otherwise}
\end{cases}
$$

Sites can be visited in many different ways.
If the user typed the entire link themselves or if it was a bookmarked link, we want to weight that differently to visiting a page by clicking a link.
Other visit types, like some types of redirects, should not be worth any score at all.
We implement this by scaling the recency score with a type weight:

$$
\operatorname{type}(v) = \begin{cases}
	1.2 & \text{if normal visit} \\
	2   & \text{if link was typed out} \\
	1.4 & \text{if link is bookmarked} \\
	0   & \text{otherwise}
\end{cases}
$$

Now that we can assign a score to every visit, we could determine the full points for a page by summing up the scores of each visit to that page.
This approach has several disadvantages.
For one, it would scale badly because some pages are visited a lot.
Additionally, user preferences change over time and we might want to decrease the points in some situations.

Instead, we compute the average score of the last 10 visits.
This score is then scaled by the total number of visits.
The full frecency score can now be efficiently computed and changes in user behaviour can be reflected fairly quickly.
Let \\(S_x\\) be the set of all visits to page \\(x\\), and let \\(T_x\\) be the set of the last 10 of these.
The full frecency score is then given by:

$$
\operatorname{frecency}(x) = \frac{|S_x|}{|T_x|} * \sum\limits_{v \in T_x} \operatorname{visit}(v)
$$

Note that this is a simplified version of the algorithm.
There is some additional logic for special cases, like typing out bookmarks or different kinds of redirects.
The description here only shows the essence of the algorithm in a mathematical form.

### Optimizing Frecency

While frecency has been working pretty well in Firefox, the weights in the algorithm were not decided on in a data-driven way.
Essentially, they are similar to [magic numbers](https://en.wikipedia.org/wiki/Magic_number_(programming)).
They were chosen because they seemed reasonable but there is no evidence showing that these numbers should be the ones being used.
Maybe different time buckets or different weights would lead to much better results.

Our project replaces the constants by variables that we optimize for.
This is done for all numbers in the previous section, except for two:
- number of considered visits (10): If this number increases too much, it would hurt performance. The current value represents a good trade-off between performance and using a sufficient amount of information
- number of time buckets (5): Optimizing this would be harder and with the current implementation there was also no easy way of changing this value

There are 22 remaining weights in the full algorithm that are optimized using our project.
By doing this, more optimal values can be found, or it can at least be confirmed that the current ones were already chosen very well.
It is also a safe way of experimenting with the Firefox URL bar:
We start our optimization process from the current set of values and then try to improve them from there.

The optimization process is based on the users' interaction with the URL bar.
They are shown a set of suggestions that were ranked using our model.
If they do not choose the top one, our model can use this feedback as a signal that it needs to change the weights to improve the rank of the selected item.
Even if the top item was chosen, we can teach our model to be more confident in this decision.

### SVM Ranking

For describing this goal formally, we need to have a loss function that evaluates how well our model did.
To this end, we use an adapted form of the SVM loss.
Essentially, we are given a set of items with their assigned score and the index of the selected item.
The optimization goal is that the selected item should have the highest score.

But even if that was the case, our model might not have been to sure in that decision.
One example for this is the selected item having a score of 100 and the second item having a score of 99.9.
The model made the correct prediction, but only barely so.
To make sure it does a good job in similar cases, we need to give a signal to the model that it can still improve.

If the URL bar displayed the suggestions for pages \\(x_1, \dots, x_n\\) in that order and suggestion \\(x_i\\) was chosen, then the SVM loss for ranking is given by:

$$
\operatorname{loss} = \sum\limits_{j \neq i} \max(0, f(x_j) + \Delta - f(x_i))
$$

We iterate over all suggestions that were not chosen and check that their score was smaller than the one of the selected page by at least a margin of \\(\Delta\\).
If not, an error is added.
The full loss should be minimized.
The margin \\(\Delta\\) is a hyperparameter that needs to be decided on before the optimization process starts.

### Computing Gradients

Every time a user performes a history or bookmark search in the URL bar, we compute the SVM loss on that search.
To compute an update, we then try to move the weights a little bit into a direction where this loss is minimized.
The update corresponds to the gradient of the SVM loss with respect to the weights in the frecency algorithm that we optimize for.

Gradients can be computed elegantly using [computational graphs](http://colah.github.io/posts/2015-08-Backprop/).
By using machine learning libraries, we first construct the function we want to compute.
Afterwards, we can make use of [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) techniques to obtain the gradient.
Our initial prototyping was based on this idea.
The major advantage is that it is very easy to change the model architecture.

The frecency implementation, however, is written in C++, while the client-side part of this experiment works using JavaScript.
This made it hard to add a computational graph to the model that was shipped to users.
Reimplementing the full algorithm in JavaScript seemed like a bad idea.
Performance-wise there would be a huge penalty and it is hard to reconstruct the way the current implementation handles all the edge cases.

Instead, we decided to use a simple [finite-difference](https://en.wikipedia.org/wiki/Finite_difference) technique.
If we want to compute the gradient of a function \\(g\\) at the point \\(x\\), we check the difference of values close to that point:

$$
g'(x) \approx \frac{g(x + \epsilon) - g(x - \epsilon)}{2 * \epsilon}
$$

This formula is very close to the definition of derivatives.
To compute the gradient of a multivariate function, this process is then performed by iterating through all dimensions independently.
In each dimension, the value is changed by \\(\epsilon\\) in the two directions, while all other values stay constant.
The resulting vector is our gradient estimate.

This method is both easy to understand and implement.
It is simple to change the frecency weights in our experiment without changing the actual algorithm.
For large models there is a performance penalty since we need to evaluate \\(g\\) two times for every dimension.
In \\(n\\) dimensions, there are \\(\mathcal{O}(n)\\) forward passes as opposed to \\(\mathcal{O}(1)\\) for computational graphs.
But since we only work in 22 dimensions here, this is not a major problem.

The finite-difference method also allows us to essentially treat frecency as a black box.
Our model does not need to know about all edge cases.
It is sufficient for the model to see how different decisions affect the output.

### Data Pipeline

The addon we built observes how users are interacting with the URL bar and retrieves all necessary information to compute the gradient.
That update together with some statistics about how well the model is doing are then sent to a Mozilla server.
This works by using the [Telemetry](https://wiki.mozilla.org/Telemetry) system, which has several advantages.
It is a well-designed system with clear rules about what can be collected.
There is a lot of infrastructure around using it and dealing with the data on the server.

All messages sent by clients are stored in a [Parquet](https://parquet.apache.org/) data store.
Every 30 minutes, a [Spark](https://spark.apache.org/) jobs reads the new updates and averages them.
The average update is then given to an optimizer and applied to the update.
The resulting model is published and fetched by clients.

### Updating the Model

One central problem with applying the update to the model is choosing the hyperparameters of the optimizer.
Since we did not collect any data, it is hard to tune the optimizer beforehand.
Even values like the learning rate are hard to set since we have no information about the gradient magnitude.
Trying out many different learning rates in production would take time and could lead to a bad user experience.
Directly collecting some data conflicts with the goal of doing machine learning without collecting data.

We tackled this problem in two ways.
First of all, we created simulations that use a made-up dataset that should be similar to the one we expect to see in production.
This allowed experimenting with different optimizers and helped with making early design decisions.
It also made it possible to quickly iterate on ideas to see if they could work.

The second way of dealing with the fact that it is hard to set hyperparameters was using the [RProp](https://florian.github.io/rprop/) optimizer.
This optimizer has major advantages in our case:
- It completely ignores the gradient magnitude and only takes into account the sign of the gradient. This means it will work with any sort of gradient we could see in production. We do not have to worry about properly scaling it
- It automatically adapts internal learning rates based on how well they work. So even if the initial values are off, they will move to decent ones in a few iterations
- The updates produced by RProp are very interpretable. In our case, we make sure they are 3 at most, so that frecency scores only change slowly

After RProp produces an update, we still apply several constraints to it.
- Weights have to be nonnegative. This means visiting a site can not directly have a negative effect
- The weights for the time buckets have to be in order. A recent visit has to be more valuable than a visit 90 days ago

These essentially act as safeguards to make sure that user experience does not degrade too much if the optimization process fails.

### Study

Users in the experiment were split into three groups:

- *treatment*: The full study was shipped to these users. They compute updates, send them to the server, and start using a new model every 30 minutes
- *control*: This group is solely observatory. No behaviour in the URL bar actually changes. We are just collecting statistics for comparison to treatment
- *control-no-decay*: Firefox decays frecency scores over time. Our treatment group loses this effect because we are recomputing scores every 30 minutes. To check if the decay is actually useful, this group has no decay effect but the same original algorithm otherwise

The study was shipped to 500,000 people.
60% of these were in the treatment group, while the other 40% were split among two control groups.

### Metrics

To evaluate how well our trained model works, we had three success criteria:
1. Do not significantly decrease the quality of the existing Firefox URL bar
2. Successfully train a model using Federated Learning
3. Stretch goal: Improve the Firefox URL bar

Actually improving the quality of the ranking for users was only a stretch goal.
The primary goal of the study was to see if it is possible to make the distributed optimization process work.
Essentially this meant consistently decreasing the loss of the model.
At the same time, the quality of the URL bar should not decrease.
The reason for distinguishing between these is that our optimization goal could have been misaligned.
It is possible to minimize some loss function without actually improving the experience for the user.

To measure the quality of history and bookmark suggestions in the URL bar, we used two metrics:
- The rank of the suggestion that was selected: The item that is selected should be as far on top as possible
- Number of characters typed before selecting a result: Users should have to type few characters to find what they are looking for

If the quality of any of these two metrics increases, we consider the stretch goal to be reached.
We were not entirely sure if both metrics could be increased.
One theory for this was that maybe users always type a similar number of characters before choosing one the suggestions.
The alternative could also be possible, users always type until the first suggestion displayed is the one they were looking for.
For this reason we decided that for the third goal only of the metrics would need to be improved.
The first goal meant that both metrics should not get significantly worse.

### Power Analysis

[Power Analysis](https://en.wikipedia.org/wiki/Power_(statistics)) is an important part of designing studies.
It tries to answer the question of how many people are required to get statistically significant results in an experiment.
If too few people are enrolled, the results will contain too much random noise to rely on them.
If a lot of people are enrolled, we can be confident in the results but the cost of the study will be much higher.

In the case of Firefox, this cost consists of two factors.
For one, if our study enrolls most Firefox users, we would block other studies that want to experiment with changes in the URL bar.
Another reason is that the experiment might break parts of Firefox.
If this happens, it should not affect unnecessarily many people.

For this reason, we performed a power analysis to decide on our sample sizes for treatment and control.
Concretely, this analysis consisted of two parts:
1. How many users do we need to have enough data to train a model? <br> (relevant for treatment)
2. How many users do we need to show certain effects confidently? <br> (relevant for treatment and control)

The first part was answered using simulations.
By using an adapted form of the simulation we used to decide on optimization hyperparameters, we could get some idea on how many users we would need.
Existing Telemetry data was helpful for this, as it allowed us to get some idea of how many history searches people perform every day.

The second part was analyzed using classical hypothesis testing.
For most metrics we track, proportion tests and t-tests were sufficient.
To analyze the rank of the selected item, we made use of the [Mann-Whitney-U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test).
This analysis turned out to be pretty helpful since we realized that our control groups could be smaller than the treatment group.

### Analyzing the Results

- Statistics: How many pings, clients. In total and per iteration
- Did metrics improve?
- How fast was convergence?
- How many data points did we need per iteration to have stable results?
- How does beta activity vary over the course of a day?

### Future Work

- More sophisticated frecency optimization
- Language model
- FL for other applications in Firefox

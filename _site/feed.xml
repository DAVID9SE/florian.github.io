<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Florian Hartmann</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 29 Sep 2018 20:41:59 +0200</pubDate>
    <lastBuildDate>Sat, 29 Sep 2018 20:41:59 +0200</lastBuildDate>
    <generator>Jekyll v3.8.3</generator>
    
      <item>
        <title>Federated Learning for Firefox</title>
        <description>&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;em&gt;To improve the Firefox URL bar, we used Federated Learning, a new privacy-preserving machine learning technique.
Roughly 360,000 users helped to train and evaluate a model in a distributed way, and our results show that this optimization process worked well.
This posts explains the decisions we made and shows the study results.
Since Federated Learning is still a young technique, this is one of the very first implementations in a major software project.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Most of machine learning nowadays is based on collecting a lot of data which is then put on a powerful server where the training is performed.
If the data is considered private by people, using machine learning this way is not possible or a bad idea.
The entirely opposite approach, a completely decentralized system where individual models are trained for each user locally, generally works badly.
Individuals by themselves often do not have enough data to fit good models.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Federated Learning&lt;/em&gt; is a new subarea of machine learning where the training process is distributed.
The parts of the algorithm that touch the users’ data are executed locally on their machines.
Instead of sharing data, clients then only send back abstract model improvements, for example weight updates, to the server.
This way, a high-quality model can be trained collaboratively without needing to collect data.
The &lt;a href=&quot;/federated-learning/&quot;&gt;Federated Learning blog post&lt;/a&gt; on this site provides a much broader overview of this area.&lt;/p&gt;

&lt;p&gt;Over the past three months, we implemented a Federated Learning system for Firefox.
The aim of this project was to improve part of the suggestions displayed in the Firefox URL bar, though in theory the system is flexible enough to be used in many other situations.
Collecting local search queries and browsing histories is extremely bad for privacy, so using Federated Learning to still train a model on this data makes a lot of sense.
Initially, we experimented with &lt;a href=&quot;https://en.wikipedia.org/wiki/Language_model&quot;&gt;language models&lt;/a&gt; but then reduced the scope of the project to make it possible to implement everything in three months.&lt;/p&gt;

&lt;h3 id=&quot;search-in-the-firefox-url-bar&quot;&gt;Search in the Firefox URL bar&lt;/h3&gt;

&lt;p&gt;The Firefox URL bar shows suggestions when users type a search query.
A part of these suggestions is provided directly by a search engine.
The others are generated by Firefox itself, for example based on the user’s history, bookmarks or open tabs.
We tried to optimize the history and bookmark suggestions using our project.&lt;/p&gt;

&lt;table class=&quot;image&quot;&gt;
	&lt;caption align=&quot;bottom&quot; style=&quot;margin-top:0&quot;&gt;The Firefox URL bar with some suggestions based on the browsing history&lt;/caption&gt;
	&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/assets/posts/federated-learning-firefox/bar.png&quot; alt=&quot;The Firefox URL bar with some suggestions based on the browsing history&quot; width=&quot;&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Searching for history and bookmark entries in the Firefox URL bar is a two-step process:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The search query is matched against the browser history and bookmarks. Matching is a binary decision. Pages either match the query or do not&lt;/li&gt;
  &lt;li&gt;The set of matched links is ranked based on the user’s history&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Our project purely tries to optimize the ranking part of this process.
Future work could tackle the problem directly from the query matching.&lt;/p&gt;

&lt;h3 id=&quot;learning-to-rank&quot;&gt;Learning to Rank&lt;/h3&gt;

&lt;p&gt;Before diving into the current implementation, it is worth taking a step back to understand how ranking in machine learning works.
This makes it easier to see how the current algorithm fits into a machine learning system.
Fundamentally, there are three different approaches to learning a ranking algorithm:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Pointwise ranking&lt;/em&gt;: Each item is given separately to the model, which assigns a score to the item. The ranking is then determined by sorting all items using their respective scores. Essentially, this is a special type of a regression model since we are assigning a real number to every input&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Pairwise ranking&lt;/em&gt;: The model learns to compare pairs of items. Its task is to decide which of the two items should be ranked higher. Intuitively, this method can be motivated by the fact that the learned comparison function could then be used by various sorting algorithms. In this approach, we treat the problem as a classification task since the model can only have two possible outputs&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Listwise ranking&lt;/em&gt;: Instead of only working with individual items in each step, these methods try to operate on the entire list. The motivation behind this idea is that the evaluation metric can be optimized directly. In practice, this turns out to be fairly difficult because many evaluation metrics are not differentiable and the models need to work with many more inputs. Another difficulty is that the list could have an arbitrary length&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All these approaches have different advantages and disadvantages.
The existing ranking algorithm in Firefox is very similar to a pointwise ranking approach.
Since this algorithm should be optimized using machine learning techniques, this gives us a clear set of techniques that could be useful in this project.&lt;/p&gt;

&lt;h3 id=&quot;frecency&quot;&gt;Frecency&lt;/h3&gt;

&lt;p&gt;The ranking of possible suggestions in the Firefox URL bar is determined using &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Mozilla/Tech/Places/Frecency_algorithm&quot;&gt;&lt;em&gt;frecency&lt;/em&gt;&lt;/a&gt;, an algorithm that weights how &lt;em&gt;frequently&lt;/em&gt; and &lt;em&gt;recently&lt;/em&gt; a site was visited.
To do this, a frecency score is assigned to each history entry and bookmark entry.
After computing the score, it is cached.
When searching, the matched results are then sorted using this score.
This section introduces the existing frecency algorithm, while the next one explains how we planned to improve it.&lt;/p&gt;

&lt;p&gt;Frecency does not only take frequency and recency into account but also other information, such as how the page was visited and whether it is bookmarked.
It does this by looking at the latest visits to the respective site.
The value \(\operatorname{visit}(v)\) of one single visit \(v\) is then defined by how recent that visit was, scaled by the type of visit:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\operatorname{visit}(v) = \operatorname{recency}(v) * \operatorname{type}(v)&lt;/script&gt;

&lt;p&gt;Frecency scores have to be cached in order to allow an efficient ranking while the user is typing.
This means that the recency aspect has to be modeled using time buckets.
Otherwise, the score would change all the time and caching would not work.
In the current Firefox implementation, there are five time buckets.
With this approach, the recency score only changes when a visit changes time buckets:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\operatorname{recency}(v) = \begin{cases}
	100 &amp; \text{if visited in the past 4 days} \\
	 70 &amp; \text{if visited in the past 14 days} \\
	 50 &amp; \text{if visited in the past 31 days} \\
	 30 &amp; \text{if visited in the past 90 days} \\
	 10 &amp; \text{otherwise}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;Sites can be visited in many different ways.
If the user typed the entire link themselves or if it was a bookmarked link, we want to weight that differently to visiting a page by clicking a link.
Other visit types, like some types of redirects, should not be worth any score at all.
We implement this by scaling the recency score with a type weight:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\operatorname{type}(v) = \begin{cases}
	1.2 &amp; \text{if normal visit} \\
	2   &amp; \text{if link was typed out} \\
	1.4 &amp; \text{if link is bookmarked} \\
	0   &amp; \text{otherwise}
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now that we can assign a score to every visit, we could determine the full points of a page by summing up the scores of each visit to that page.
This approach has several disadvantages.
For one, it would scale badly because some pages are visited a lot.
Additionally, user preferences change over time and we might want to decrease the points in some situations.&lt;/p&gt;

&lt;p&gt;Instead, we compute the average score of the last 10 visits.
This score is then scaled by the total number of visits.
The full frecency score can now be computed efficiently and changes in user behavior are reflected fairly quickly.
Let \(S_x\) be the set of all visits to page \(x\), and let \(T_x\) be the set of the last up to 10 of these.
The full frecency score is then given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\operatorname{frecency}(x) = \frac{|S_x|}{|T_x|} * \sum\limits_{v \in T_x} \operatorname{visit}(v)&lt;/script&gt;

&lt;p&gt;Note that this is a simplified version of the algorithm.
There is some additional logic for special cases, like typing out bookmarks or different kinds of redirects.
The description here only shows the essence of the algorithm in a mathematical form.&lt;/p&gt;

&lt;h3 id=&quot;optimizing-frecency&quot;&gt;Optimizing Frecency&lt;/h3&gt;

&lt;p&gt;While frecency has been working pretty well in Firefox, the weights in the algorithm were not decided on in a data-driven way.
Essentially, they are similar to &lt;a href=&quot;https://en.wikipedia.org/wiki/Magic_number_(programming)&quot;&gt;magic numbers&lt;/a&gt; in software engineering.
It is hard to understand why these exact numbers should be used.
Even worse, there is no evidence that they are optimal.
Maybe different time buckets or different weights would lead to much better results.&lt;/p&gt;

&lt;p&gt;Our project replaces the constants by variables that we optimize for.
This is done for all numbers in the previous section, except for two:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;number of considered visits (10): If this number increases too much, it would hurt performance. The current value represents a good trade-off between performance and using a sufficient amount of information&lt;/li&gt;
  &lt;li&gt;number of time buckets (5): Optimizing this would be difficult with an approach based on gradient descent since this value affects how many variables there are. In the current implementation there was also no easy way of changing this&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are 22 remaining weights in the full algorithm that are optimized using our project.
By doing this, more optimal values can be found, or it can at least be confirmed that the current ones were already chosen very well.
It is also a safe way of experimenting with the Firefox URL bar:
We start our optimization process from the current set of values and then try to improve them from there.&lt;/p&gt;

&lt;p&gt;The optimization process is based on the users’ interactions with the URL bar.
Each time they type into the URL bar, they are shown a set of suggestions that were ranked using our model.
If they do not choose the top one, our model can use this feedback as a signal that it needs to change the weights to improve the rank of the selected item.
Even if the top item was chosen, we can teach our model to be more confident in this decision.
By using Federated Learning, we can make use of a lot such data, without sacrificing privacy.&lt;/p&gt;

&lt;h3 id=&quot;svm-ranking&quot;&gt;SVM Ranking&lt;/h3&gt;

&lt;p&gt;To describe this goal formally, we need to have a loss function that evaluates how well our model did.
To this end, we make use of a variant of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Hinge_loss&quot;&gt;SVM loss&lt;/a&gt;, which we adapted for pointwise ranking.
Essentially, the loss function takes a set of items with their assigned score and the index of the selected item.
The optimization goal is that the selected item should have the highest score.&lt;/p&gt;

&lt;p&gt;But even if that was the case, our model might not have been too confident in that decision.
One example of this is the selected item having a score of 100 and the second item having a score of 99.9.
The model made the correct prediction, but only barely so.
To make sure it does a good job in similar cases, we need to provide a signal to the model which shows that it can still improve.&lt;/p&gt;

&lt;p&gt;If the URL bar displayed the suggestions for pages \(x_1, \dots, x_n\) in that order and suggestion \(x_i\) was chosen, then the SVM loss for pointwise ranking is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\operatorname{loss} = \sum\limits_{j \neq i} \max(0, f(x_j) + \Delta - f(x_i))&lt;/script&gt;

&lt;p&gt;We iterate over all suggestions that were not chosen and check that their score was smaller than the one of the selected page by at least a margin of \(\Delta\).
If not, an error is added.
The full loss should be minimized.
The margin \(\Delta\) is a hyperparameter that needs to be decided on before the optimization process starts.&lt;/p&gt;

&lt;table class=&quot;image&quot;&gt;
	&lt;caption align=&quot;bottom&quot; style=&quot;&quot;&gt;A visualization of the SVM loss when adapted for ranking&lt;/caption&gt;
	&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/assets/posts/federated-learning-firefox/svm-loss.png&quot; alt=&quot;A visualization of the SVM loss when adapted for ranking&quot; width=&quot;500&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The image above shows a visualization of this loss function.
Each bar represents a possible suggestion, with the selected one being shown in black.
The y-axis displays how many points the model assigned to the respective suggestion.
The hatched areas show the SVM loss.
Everything above the selected suggestion and below it by a margin of \(\Delta\) adds to the full loss.
Even though the selected suggestion had the second highest score, four suggestions contribute to the penalty.&lt;/p&gt;

&lt;h3 id=&quot;computing-gradients&quot;&gt;Computing Gradients&lt;/h3&gt;

&lt;p&gt;Every time a user performs a history or bookmark search in the URL bar, we compute the SVM loss on that search.
To compute an update, we then try to move the weights a little bit into a direction where this loss is minimized.
The update corresponds to the gradient of the SVM loss with respect to the weights in the frecency algorithm that we optimize for.&lt;/p&gt;

&lt;p&gt;Gradients can be computed elegantly using &lt;a href=&quot;http://colah.github.io/posts/2015-08-Backprop/&quot;&gt;computational graphs&lt;/a&gt;.
By using machine learning libraries, we first construct the function we want to compute.
Afterwards, we can make use of &lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_differentiation&quot;&gt;automatic differentiation&lt;/a&gt; techniques to obtain the gradient.
Our initial prototyping was based on this idea.
The major advantage is that it is very easy to change the model architecture.&lt;/p&gt;

&lt;p&gt;The current frecency implementation in Firefox, however, is written in C++, while the client-side part of this experiment works using JavaScript.
To launch the experiment, the Firefox &lt;a href=&quot;https://wiki.mozilla.org/Firefox/Shield/Shield_Studies&quot;&gt;SHIELD&lt;/a&gt; mechanism was used.
SHIELD allows us to directly send new code to Firefox clients, without having to go through major version releases, which only happen every few weeks.
To be able to do this, SHIELD experiments have to be written in JavaScript and can only make very limited use of C++ components.&lt;/p&gt;

&lt;p&gt;This made it hard to add a computational graph to the existing C++ frecency module.
Reimplementing the full algorithm in JavaScript seemed like a bad idea.
Performance-wise there would be a huge penalty and it is hard to reconstruct the way the current implementation handles all the edge cases.&lt;/p&gt;

&lt;p&gt;Instead, we decided to use a simple &lt;a href=&quot;https://en.wikipedia.org/wiki/Finite_difference&quot;&gt;finite-difference&lt;/a&gt; technique.
If we want to compute the gradient of a function \(g\) at the point \(x\), we check the difference of values close to that point:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g'(x) \approx \frac{g(x + \epsilon) - g(x - \epsilon)}{2 * \epsilon}&lt;/script&gt;

&lt;p&gt;This formula is very close to the definition of derivatives.
To compute the gradient of a multivariate function, this process is then performed by iterating through all dimensions independently.
In each dimension, the value is changed by \(\epsilon\) in the two directions, while all other values stay constant.
The resulting vector is our gradient estimate.&lt;/p&gt;

&lt;p&gt;This method is both easy to understand and implement.
It is simple to change the frecency weights in our experiment without changing the actual algorithm.
For large models there is a performance penalty since we need to evaluate \(g\) two times for every dimension.
In \(n\) dimensions, there are \(\mathcal{O}(n)\) function evaluations as opposed to \(\mathcal{O}(1)\) for computational graphs.
But since we only work in 22 dimensions here, this is not a major problem.&lt;/p&gt;

&lt;p&gt;The finite-difference method also allows us to essentially treat frecency as a black box.
Our model does not need to know about all edge cases.
It is sufficient for the model to see how different decisions affect the output and the loss.&lt;/p&gt;

&lt;h3 id=&quot;data-pipeline&quot;&gt;Data Pipeline&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;http://github.com/florian/federated-learning-addon&quot;&gt;addon&lt;/a&gt; we built observes how users are interacting with the URL bar and retrieves all necessary information to compute the gradient.
That update and some statistics about how well the model is doing are then sent to a Mozilla server.
This works by using the &lt;a href=&quot;https://wiki.mozilla.org/Telemetry&quot;&gt;Telemetry&lt;/a&gt; system, which has several advantages.
It is a well-designed system with clear rules about what can be collected.
There is a lot of infrastructure around using it and dealing with the data on the server.&lt;/p&gt;

&lt;p&gt;All messages sent by clients are stored in a &lt;a href=&quot;https://parquet.apache.org/&quot;&gt;Parquet&lt;/a&gt; data store.
 A &lt;a href=&quot;https://spark.apache.org/&quot;&gt;Spark&lt;/a&gt; MapReduce job reads the new updates and averages them in realtime.
Every 30 minutes, the average update is then given to an optimizer and applied to the model.
The resulting model is published to S3 and fetched by clients.&lt;/p&gt;

&lt;h3 id=&quot;updating-the-model&quot;&gt;Updating the Model&lt;/h3&gt;

&lt;p&gt;One central problem with applying the update to the model is choosing the hyperparameters of the optimizer.
Since we did not collect any data, it is hard to tune the optimizer beforehand.
Even values like the learning rate are hard to set since we have no information about the gradient magnitude.
Trying out many different learning rates in production would take time and could lead to a bad user experience.
Directly collecting some data conflicts with the goal of doing machine learning in a privacy-respecting way.&lt;/p&gt;

&lt;p&gt;We tackled this problem in two ways.
First of all, we created &lt;a href=&quot;https://github.com/florian/federated-learning&quot;&gt;simulations&lt;/a&gt; which use a made-up dataset that should be similar to the one we expect to see in production.
This allowed experimenting with different optimizers and helped with making early design decisions.
It also made it possible to quickly iterate on ideas to see if they could work.&lt;/p&gt;

&lt;p&gt;The second way of dealing with the fact that it is hard to set hyperparameters was using the &lt;a href=&quot;https://florian.github.io/rprop/&quot;&gt;RProp&lt;/a&gt; optimizer.
This optimizer has major advantages in our case:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It completely ignores the gradient magnitude and only takes into account the signs of the gradient. This means it will work with any sort of gradient we could see in production. We do not have to worry about properly scaling it&lt;/li&gt;
  &lt;li&gt;It automatically adapts internal learning rates based on how well they work. So even if the initial values are off, they will move to decent ones in a few iterations&lt;/li&gt;
  &lt;li&gt;The updates produced by RProp are very interpretable. In our case, we make sure they are 3 at most, so that frecency scores only change slowly&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After RProp produces an update, we still apply several constraints to it.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Weights have to be nonnegative. This means visiting a site cannot directly have a negative effect&lt;/li&gt;
  &lt;li&gt;The time buckets have to be sorted by the last day they take into account. In other words, the \((i + 1\))-th time bucket needs to contain older visits than the \(i\)-th time bucket. This is to ensure that the client-side frecency implementation continues to work&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These essentially act as safeguards to make sure that user experience does not degrade too much if the optimization process fails.&lt;/p&gt;

&lt;h3 id=&quot;study&quot;&gt;Study&lt;/h3&gt;

&lt;p&gt;Users in the experiment were split into three groups:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;treatment&lt;/em&gt;: The full study was shipped to these users. They compute updates, send them to the server, and start using a new model every 30 minutes&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;control&lt;/em&gt;: This group is solely for observational purposes. No behavior in the URL bar actually changes. We are just collecting statistics for comparison to treatment&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;control-no-decay&lt;/em&gt;: Firefox decays frecency scores over time. Our treatment group loses this effect because we are recomputing scores every 30 minutes. To check if the decay is actually useful, this group has no decay effect but uses the same original algorithm otherwise&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The study was shipped to 25% of Firefox Beta users who had Telemetry enabled.
As with all other SHIELD studies, only users with Telemetry enabled were eligible for enrollment.
60% of the users in our study were assigned to the treatment group, while the other 40% were split among the two control groups.&lt;/p&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;p&gt;To evaluate how well our trained model works, we had three success criteria:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Do not significantly decrease the quality of the existing Firefox URL bar&lt;/li&gt;
  &lt;li&gt;Successfully train a model using Federated Learning&lt;/li&gt;
  &lt;li&gt;Stretch goal: Improve the Firefox URL bar&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Actually improving the quality of the ranking for users was only a stretch goal.
The primary goal of the study was to see if it is possible to make the distributed optimization process work.
Essentially this meant consistently decreasing the loss of the model.
At the same time, the quality of the URL bar should not decrease.
The reason for distinguishing between these is that our optimization goal could have been misaligned.
It is possible to minimize some loss function without actually improving the experience for the user.&lt;/p&gt;

&lt;p&gt;To measure the quality of history and bookmark suggestions in the URL bar, we used two metrics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Number of characters typed before selecting a result: Users should have to type few characters to find what they are looking for&lt;/li&gt;
  &lt;li&gt;The rank of the suggestion that was selected: The item that is selected should be as far on top as possible&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If the quality of any of these two metrics increases, we consider the stretch goal to be reached.
We were not entirely sure if both metrics could be improved.
One theory for this was that maybe users always type a similar number of characters before choosing one of the suggestions.
The alternative could also be possible, users always type until the first suggestion displayed is the one they were looking for.
For this reason, we decided that for the third goal only one of the metrics would need to be improved, while the other should not get much worse.
The first goal meant that both metrics should not get significantly worse.&lt;/p&gt;

&lt;h3 id=&quot;power-analysis&quot;&gt;Power Analysis&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Power_(statistics)&quot;&gt;Power Analysis&lt;/a&gt; is an important part of designing studies.
It tries to answer the question of how many people are required to get statistically significant results in an experiment.
If too few people are enrolled, the results will contain too much random noise to rely on them.
If a lot of people are enrolled, we can be confident in the results but the cost of the study will be much higher.&lt;/p&gt;

&lt;p&gt;In the case of Firefox, this cost consists of two factors.
For one, if our study enrolls most Firefox users, we would block other studies that want to experiment with changes in the URL bar.
Another reason is that the experiment might break parts of Firefox.
If this happens, it should not affect unnecessarily many people.&lt;/p&gt;

&lt;p&gt;For this reason, we performed a power analysis to decide on our sample sizes for treatment and control.
Concretely, this analysis consisted of two parts:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;How many users do we need to have enough data to train a model? &lt;br /&gt; (relevant for treatment)&lt;/li&gt;
  &lt;li&gt;How many users do we need to show certain effects confidently? &lt;br /&gt; (relevant for treatment and control)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first part was answered using simulations.
By using an adapted form of the simulation we used to decide on optimization hyperparameters, we could get some idea on how many users we would need.
Existing Telemetry data was helpful for this, as it allowed us to get some idea of how many history searches people perform every day.&lt;/p&gt;

&lt;p&gt;The second part of the power analysis was tackled using classical hypothesis testing.
There was no prior data on the number of typed characters, so no power analysis was possible for this metric.
To analyze the rank of the selected item, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&quot;&gt;Mann-Whitney-U test&lt;/a&gt; was used since the data was not following a distribution that allows for parametric tests.
The Mann-Whitney-U test is non-parametric, which means that it does not make any assumptions about the underlying distribution of the data.
This analysis turned out to be pretty helpful since we realized that our control groups could be smaller than the treatment group.&lt;/p&gt;

&lt;h3 id=&quot;analyzing-the-results&quot;&gt;Analyzing the Results&lt;/h3&gt;

&lt;p&gt;Over the course of the experiment, 723,581 users were enrolled in the study.
The model was fetched 58,399,063 times from the server.
360,518 users participated in sending updates and evaluation data to the server, accounting for a total of 5,748,814 messages.
The optimization phase of the experiment consisted of 137 iterations of 30 minutes each, or just under three days.
In this phase, 186,315 users sent pings to help in the training process.&lt;/p&gt;

&lt;p&gt;A separate phase of purely evaluating the model was started afterwards and took a total of 10 days.
In this phase, 306,200 users send 3,674,063 pings, which included statistics detailing how well the model worked for them.
Since all these users were assigned to treatment or control groups, the new model can be compared well to the old one that was used by the control groups.
Some users were enrolled but did not help with optimization or evaluation because they performed no history and bookmark searches.&lt;/p&gt;

&lt;p&gt;During the optimization process, the loss of the model was supervised to check how well the training was going.
The plot below shows how the loss changed over time, across all three study variations.
There is some noise in this plot, since each iteration only had a very limited number of users.
However, it can still be seen that the loss of the treatment group goes down continuously.
This shows that the optimization process generally worked.
After 40 iterations, less than one day of optimization, the loss of the treatment group is significantly below the loss of the control groups.
The second goal of the study was thus reached.&lt;/p&gt;

&lt;table class=&quot;image&quot;&gt;
	&lt;caption align=&quot;bottom&quot; style=&quot;&quot;&gt;Rolling average of reported loss over the last 5 iterations&lt;/caption&gt;
	&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/assets/posts/federated-learning-firefox/loss-smooth5.png&quot; alt=&quot;Rolling average of reported loss over the last 5 iterations&quot; width=&quot;&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;After the optimization process ended, an evaluation phase began to determine how well the new model works.
This is equivalent to the testing phase in machine learning.
The model is evaluated on new data that was not used for training or validation.
The table below shows the results.
On average, users in the treatment group type about half a character less to find what they are looking for.
This is a strong improvement over both control groups.
However, users in the treatment group also choose suggestions that were ranked slightly worse.
Hypothesis testing determined that the changes in the treatment group were highly significant, with p-values being below 1e-75.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;study variation&lt;/th&gt;
      &lt;th&gt;mean characters typed&lt;/th&gt;
      &lt;th&gt;mean selected rank&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;treatment&lt;/td&gt;
      &lt;td&gt;3.6747&lt;/td&gt;
      &lt;td&gt;0.37435&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;control&lt;/td&gt;
      &lt;td&gt;4.26239&lt;/td&gt;
      &lt;td&gt;0.3535&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;control-no-decay&lt;/td&gt;
      &lt;td&gt;4.24125&lt;/td&gt;
      &lt;td&gt;0.35771&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;From a user perspective, it is not clear if these changes improve the user experience.
While users now have to type a good amount less, they also selected suggestions that were not on top of the list more often.
One potential explanation for this could be that the items they were looking for are displayed earlier in the suggestion list.
Since they spent less time typing, they might be willing to take the time to select an item that is not the top ranked one.&lt;/p&gt;

&lt;p&gt;It is hard to determine purely based on these two metrics if this change is good, since it is not clear how their importance should be weighted.
Instead, surveying users would be required to decide if goal 3 was met.
But even if users are not satisfied with the new model, the Federated Learning system is still highly useful.
Since the optimization process works well, one would only need to find a loss function that correlates more closely with what users want.
We consider goal 1 to be reached since at least one of the metrics improved.&lt;/p&gt;

&lt;p&gt;To learn from this experiment for further Federated Learning studies, we additionally analyzed all the update data later on.
In retrospect, the Federated Learning protocol we used was too simple.
The plot below shows how Firefox beta activity in our study varies over time.
The protocol could be improved by dynamically determining the iteration length depending on how many updates were sent to the server so far.
This way, there would be no iterations with very few updates.
Furthermore, there could be more iterations during periods with many active users, allowing for a faster optimization process.&lt;/p&gt;

&lt;table class=&quot;image&quot;&gt;
	&lt;caption align=&quot;bottom&quot; style=&quot;&quot;&gt;The number of pings sent by clients over time&lt;/caption&gt;
	&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/assets/posts/federated-learning-firefox/pings-over-time.png&quot; alt=&quot;The number of pings sent by clients over time&quot; width=&quot;&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;A more sophisticated protocol could adapt the iteration length depending on how stable the current update estimate is.
We noticed that the later iterations of the optimization process require much fewer reports to compute a good estimate.
The plot below compares the update we actually used to updates we would get by randomly sampling 2,000 of these update reports.
The \(L_1\)-distance is used to perform this comparison.
Because of the randomness, the mean and standard deviation after 50 such simulations per iteration are reported.&lt;/p&gt;

&lt;table class=&quot;image&quot;&gt;
	&lt;caption align=&quot;bottom&quot; style=&quot;&quot;&gt;Mean and std deviation of difference in update quality when using 2,000 updates&lt;/caption&gt;
	&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/assets/posts/federated-learning-firefox/update-quality.png&quot; alt=&quot;Mean and std deviation of difference in update quality when using 2,000 updates&quot; width=&quot;&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;It can be observed that the estimates become much more stable after iteration 100.
While the \(L_1\)-distance of two updates can be large without affecting the RProp optimizer much, this is still an interesting result.
We observed similar results for the loss estimates.&lt;/p&gt;

&lt;h3 id=&quot;future-work&quot;&gt;Future Work&lt;/h3&gt;

&lt;p&gt;There is still a lot of low-hanging fruit in this Federated Learning system.
For one, the protocol could be made much more sophisticated, as explained above.
Furthermore, when trying to interpret the new weights, it seems like it would be a good idea to introduce additional weight constraints to make better use of the time buckets.&lt;/p&gt;

&lt;p&gt;To be able to implement the entire system in three months, a lot of simplifications had to be done.
This is why our resulting model has few weights and is easy to interpret.
If more time could go into this project, a language model would be interesting because it would allow tackling the problem directly from a query matching angle.
Instead of making suggestions based on the current query, we could base them on what the model thinks the user is going to type next.&lt;/p&gt;

&lt;p&gt;Finally, there are other applications for Federated Learning in Firefox.
Based on user interactions with Firefox, our system could really optimize anything that currently relies on hardcoded constants.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;open-source-code&quot;&gt;Open Source Code&lt;/h3&gt;

&lt;p&gt;Nearly the entire code of the Federated Learning system is open sourced:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://github.com/florian/federated-learning-addon&quot;&gt;Firefox addon&lt;/a&gt; that implements the client-side part of the system&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/mozilla/telemetry-streaming/tree/master/src/main/scala/com/mozilla/telemetry/learning/federated&quot;&gt;Server-side implementation&lt;/a&gt; which computes the next model&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/florian/federated-learning&quot;&gt;Simulations&lt;/a&gt; for experimenting with Federated Learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Only a few analyses based on Firefox interaction data are not publicly available.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;thanks&quot;&gt;Thanks!&lt;/h3&gt;

&lt;p&gt;This project would not have been possible without the help of many people:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://sunahsuh.com/&quot;&gt;Sunah Suh&lt;/a&gt; planned out the project with me and helped to resolve many blockers along the way&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/akkomar&quot;&gt;Arkadiusz Komarzewski&lt;/a&gt; ported the Python optimizer to Scala and implemented the remaining server-side logic&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/0c0w3&quot;&gt;Drew Willcoxon&lt;/a&gt; and &lt;a href=&quot;https://rhelmer.org/&quot;&gt;Rob Helmer&lt;/a&gt; helped a lot with Firefox specific parts of the project&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://tds.xyz/&quot;&gt;Tim Smith&lt;/a&gt; provided most of the code for using the Mann-Whitney-U test and along with &lt;a href=&quot;https://github.com/ilanasegall&quot;&gt;Ilana Segall&lt;/a&gt; answered many questions related to power analyses&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jeff.klukas.net&quot;&gt;Jeff Klukas&lt;/a&gt; always had an open ear when I ran into problems and helped out when Sunah was on vacation&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/kparlante&quot;&gt;Katie Parlante&lt;/a&gt; made it possible for me to return to Mozilla to work on this and was very open to my ideas for projects&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thanks a lot for all your help and support!&lt;/p&gt;
</description>
        <pubDate>Mon, 27 Aug 2018 15:59:59 +0200</pubDate>
        <link>http://localhost:4000/federated-learning-firefox/</link>
        <guid isPermaLink="true">http://localhost:4000/federated-learning-firefox/</guid>
        
        
        <category>machine-learning,</category>
        
        <category>federated-learning</category>
        
      </item>
    
      <item>
        <title>Estimation Theory and Machine Learning</title>
        <description>&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;Often it is not possible or simply impractical to compute certain values exactly.
This might be because it is too expensive computationally or because not enough information is available.
Instead, these values can be estimated.
The quality of estimates varies.
In statistics, this concept is formalized in estimation theory &lt;a href=&quot;#citation-1&quot; id=&quot;ref-1&quot; class=&quot;ref-link&quot;&gt;[1]&lt;/a&gt;
 &lt;a href=&quot;#citation-2&quot; id=&quot;ref-2&quot; class=&quot;ref-link&quot;&gt;[2]&lt;/a&gt;
.&lt;/p&gt;

&lt;p&gt;The first part of this blog post introduces the fundamentals behind estimators.
In the second part, it is shown how they can be applied to machine learning in two different ways.
One of these applications is quantifying the quality of models.
Since models can generally not be perfect for complex problems, it is useful to try to describe how well they work.&lt;/p&gt;

&lt;p&gt;Additionally, estimation theory is useful to understand different versions of gradient descent.
Typically, the gradient is only estimated using methods like mini-batch or stochastic gradient descent.
Here, estimation theory can be used to explain the ideas behind these techniques.&lt;/p&gt;

&lt;h3 id=&quot;estimators-and-their-properties&quot;&gt;Estimators and their properties&lt;/h3&gt;

&lt;p&gt;An &lt;em&gt;estimator&lt;/em&gt; is a function that estimates a value based on other observations.
This process can involve randomness.
For example, because the function itself is random or because there is random noise in the observations it uses.&lt;/p&gt;

&lt;h4 id=&quot;bias&quot;&gt;Bias&lt;/h4&gt;

&lt;p&gt;One measure for the quality of an estimator \(\tilde{X}\) is its &lt;em&gt;bias&lt;/em&gt; or how far off its estimate is on average from the true value \(X\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\operatorname{bias}({\tilde{X}}) = \mathbb{E}[\tilde{X}] - X&lt;/script&gt;

&lt;p&gt;where the expected value is over the randomness involved in \(\tilde{X}\).&lt;/p&gt;

&lt;p&gt;If the bias of an estimator is \(0\), it is called an &lt;em&gt;unbiased estimator&lt;/em&gt;.
This is generally a desirable property to have &lt;a href=&quot;#citation-3&quot; id=&quot;ref-3&quot; class=&quot;ref-link&quot;&gt;[3]&lt;/a&gt;
 because it means that the estimator is correct on average.
If one samples for long enough from the estimator, the average converges to the true value \(X\).
This is due to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Law_of_large_numbers&quot;&gt;law of large numbers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;: If \(k\) estimators all produce unbiased estimates \(\tilde{X}_1, \dots, \tilde{X}_k\) of \(X\), then any weighted average of them is also an unbiased estimator.
The full estimate is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tilde{X} = w_1 * \tilde{X}_1 + \ldots + w_k * \tilde{X}_k&lt;/script&gt;

&lt;p&gt;where the sum of weights \(\sum_{i = 1}^k w_i = 1\) needs to be normalized.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: The unbiasedness is due to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Expected_value#Linearity&quot;&gt;linearity of expectation&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\mathbb{E}[\tilde{X}] &amp; = \mathbb{E}[w_1 * \tilde{X}_1 + \ldots + w_k * \tilde{X}_k] \\
	              &amp; = w_1 * \mathbb{E}[\tilde{X}_1] + \ldots + w_k * \mathbb{E}[\tilde{X}_k] \\
	              &amp; = w_1 * X + \ldots + w_k * X \\
	              &amp; = X
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This theorem about unbiased estimators is going to prove to be useful later on.&lt;/p&gt;

&lt;h4 id=&quot;variance&quot;&gt;Variance&lt;/h4&gt;

&lt;p&gt;However, even if we have an unbiased estimator, its individual estimates can still be far off from the true value.
To quantify how consistently an estimator is close to the true value, another statistic is required.
Commonly, the &lt;em&gt;variance&lt;/em&gt; of the estimator is considered here:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\operatorname{Var}[\tilde{X}] = \mathbb{E}[(\tilde{X} - X)^2]&lt;/script&gt;

&lt;p&gt;It is defined as the mean squared distance between the estimate and the value to be estimated.&lt;/p&gt;

&lt;h3 id=&quot;bias-variance-tradeoff&quot;&gt;Bias-variance tradeoff&lt;/h3&gt;

&lt;p&gt;Many different things can be analyzed using estimators.
For example, statistical models can be seen as estimators.
They use observations, or data, to make predictions.
These predictions are generally not perfect because randomness is involved and only a limited amount of information is available.
Thus, it makes sense to analyze statistical models in terms of bias and variance.&lt;/p&gt;

&lt;p&gt;A central problem when building models is balancing underfitting and overfitting.
If the training data is just memorized, the model does not generalize well to new data.
This is a case of overfitting.
The opposite issue, only barely matching the pattern in the training data, is called underfitting.&lt;/p&gt;

&lt;p&gt;This problem is also known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Bias–variance_tradeoff&quot;&gt;&lt;em&gt;bias-variance tradeoff&lt;/em&gt;&lt;/a&gt; &lt;a href=&quot;#citation-4&quot; id=&quot;ref-4&quot; class=&quot;ref-link&quot;&gt;[4]&lt;/a&gt;
 &lt;a href=&quot;#citation-5&quot; id=&quot;ref-5&quot; class=&quot;ref-link&quot;&gt;[5]&lt;/a&gt;
.
If the model has a high bias, its predictions are off, which corresponds to underfitting.
If overfitting occurred, i.e. the data is matched too well, the estimates have a high variance.
By resampling the data that the model was built on, totally different estimates are generated.
This is because the model is now based on different random noise.&lt;/p&gt;

&lt;p&gt;Generally, it is not possible to perfectly optimize both, bias and variance, so they need to be balanced here.
In other words, we accept a certain bias of the model to keep its variance low.
A good tradeoff between the two needs to be achieved.&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent&quot;&gt;Gradient descent&lt;/h3&gt;

&lt;p&gt;In supervised machine learning, we compare our model’s predictions to the true labels.
This is done using a loss function.
If a set of data points \(x_1, \dots, x_n\) and labels \(y_1, \dots\, y_n\) is given, then the full loss is defined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L = \frac{1}{n} \sum\limits_{i = 1}^n \operatorname{loss}(f(x_i), y_i)&lt;/script&gt;

&lt;p&gt;where \(\operatorname{loss}\) is a function that compares a prediction \(p\) to the correct answer \(y\).
One choice for the loss function might be the quadratic error:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\operatorname{loss}(p, y) = (p - y)^2&lt;/script&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;&gt;Gradient descent&lt;/a&gt; optimizes the parameters used in \(f\) by computing the gradient of the loss with respect to these parameters.
This gradient is then used to continually improve the parameters step by step.&lt;/p&gt;

&lt;h4 id=&quot;full-batch-gradient-descent&quot;&gt;Full-batch gradient descent&lt;/h4&gt;

&lt;p&gt;To compute the gradient \(\nabla L\) of the loss, we can make use of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient#Linearity&quot;&gt;linearity of the gradient operator&lt;/a&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\nabla L &amp; = \nabla \frac1n \sum\limits_{i = 1}^n \operatorname{loss}(f(x_i), y_i) \\
	         &amp; = \frac1n \sum\limits_{i = 1}^n \nabla \operatorname{loss}(f(x_i), y_i)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The method that uses the gradient given above is sometimes referred to as &lt;em&gt;full-batch gradient descent&lt;/em&gt; because it fully uses the available training data in each iteration.
In many cases, \(n\) is a very large value and computing the full update \(\nabla L\) is expensive.
Since computing the gradient is by far the most expensive part of gradient descent, it makes sense to try to make this more efficient.&lt;/p&gt;

&lt;p&gt;Computing the gradient as shown above is especially inefficient if there is duplicated training data.
If the training set consists of 10 copies of a different dataset, then the evaluation of the formula above is unnecessarily expensive.
Every required calculation is repeated 10 times.
While this is an extreme example, it does happen in practice that much of the training data is similar.
To save time, it often makes sense to only use a part of the data to estimate the gradient.&lt;/p&gt;

&lt;h4 id=&quot;stochastic-gradient-descent&quot;&gt;Stochastic gradient descent&lt;/h4&gt;

&lt;p&gt;In &lt;em&gt;stochastic gradient descent&lt;/em&gt; (&lt;em&gt;SGD&lt;/em&gt;), a single data point \(x\) and label \(y\) are sampled uniformly from the training set.
The true gradient \(\nabla L\) is then estimated using only this data point and label:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla \tilde{L} = \nabla \operatorname{loss}(f(x), y)&lt;/script&gt;

&lt;p&gt;It is easy to see that \(\nabla \tilde{L}\) is an unbiased estimator of \(\nabla L\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
	\mathbb{E}[\nabla \tilde{L}] &amp; = \sum\limits_{i = 1}^n \frac{1}{n} \nabla \operatorname{loss}(f(x_i), y_i)  \\
	                     &amp; = \frac1n \nabla \sum\limits_{i = 1}^n \operatorname{loss}(f(x_i), y_i) \\
	                     &amp; = \nabla L
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The computations for SGD can be performed very quickly but still give us an unbiased estimate of the true gradient.
This property is the reason why optima can be found using this algorithm.
While individual estimates are off, the randomness averages out over iterations and the parameters still move in a sensible direction overall.
Since iterations are much cheaper, many more of them can be performed and this is a major improvement to computing the full gradient.&lt;/p&gt;

&lt;h4 id=&quot;mini-batch-gradient-descent&quot;&gt;Mini-batch gradient descent&lt;/h4&gt;

&lt;p&gt;These individual SGD estimates can have a large variance however, leading to noisy and jumpy updates.
A further improvement over this method is &lt;em&gt;mini-batch gradient descent&lt;/em&gt;.
Instead of just sampling one data point, we sample a small batch of \(k\) examples.
The estimated gradient is an average of all \(k\) single estimates.&lt;/p&gt;

&lt;p&gt;Each of these individual estimators is unbiased since SGD itself is unbiased.
As shown in the theorem earlier, a weighted combination of them still remains an unbiased estimator.
Thus, mini-batch gradient descent is also an unbiased way of computing gradient estimates.&lt;/p&gt;

&lt;p&gt;Mini-batch gradient descent does have much less variance, however, because more data is used to compute the estimate.
This makes the optimization process more stable compared to using SGD.&lt;/p&gt;

&lt;p&gt;Most gradient computations can be formulated using linear algebra operations.
These calculations can be parallelized very well on GPUs &lt;a href=&quot;#citation-6&quot; id=&quot;ref-6&quot; class=&quot;ref-link&quot;&gt;[6]&lt;/a&gt;
.
So with appropriate hardware there is no significant performance penalty for using \(1 &amp;lt; k \ll n\) data points to compute the estimate.
Thus mini-batch gradient descent is typically not much slower than SGD but leads to a more stable optimization process.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Estimators provide an elegant way of analyzing the quality of estimates.
In machine learning, estimates play an important role because data contains a lot of random noise and because it is often more practical to only estimate values.
The quality of statistical models can be described in terms of bias and variance.
Too much bias corresponds to underfitting, while too much variance is equivalent to overfitting.
The training process needs to find a tradeoff between these two.&lt;/p&gt;

&lt;p&gt;To compute the gradient for the optimization process, it is expensive to use all data points.
By randomly sampling them, we can compute unbiased estimates in a much faster way.
If this is done using a large enough sample, the variance of these estimates does not have to be large.
By properly choosing the sample size, the optimization process can thus be speeded up significantly.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ol class=&quot;references-list&quot;&gt;
	
	&lt;li&gt;&lt;span id=&quot;citation-1&quot;&gt;Diez, D.M., Barr, C.D. and Cetinkaya-Rundel, M., 2012. OpenIntro statistics (Vol. 12). CreateSpace. &lt;a href=&quot;#ref-1&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-2&quot;&gt;Härdle, W. and Simar, L., 2007. Applied multivariate statistical analysis (Vol. 22007, pp. 1051-8215). Berlin: Springer. Vancouver &lt;a href=&quot;#ref-2&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-3&quot;&gt;Voinov, V.G. and Nikulin, M.S., 2012. Unbiased Estimators and Their Applications: Volume 1: Univariate Case (Vol. 263). Springer Science &amp;amp; Business Media. &lt;a href=&quot;#ref-3&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-4&quot;&gt;Raul Rojas. The bias-variance dilemma. Freie University, Berlin, Tech. Rep, 2015. &lt;a href=&quot;#ref-4&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-5&quot;&gt;Friedman, J., Hastie, T. and Tibshirani, R., 2001. The elements of statistical learning (Vol. 1, No. 10). New York, NY, USA:: Springer series in statistics. &lt;a href=&quot;#ref-5&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-6&quot;&gt;Navarro, C.A., Hitschfeld-Kahler, N. and Mateu, L., 2014. A survey on parallel computing and its applications in data-parallel problems using GPU architectures. Communications in Computational Physics, 15(2), pp.285-329. &lt;a href=&quot;#ref-6&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;


&lt;/ol&gt;

&lt;script src=&quot;https://unpkg.com/tippy.js@3/dist/tippy.all.min.js&quot;&gt;&lt;/script&gt;

&lt;script&gt;
var alreadySeenRefs = {};

document.querySelectorAll(&quot;.ref-link&quot;).forEach(function (a) {
  var id = a.getAttribute(&quot;id&quot;);
  var citationPath = &quot;#&quot; + id.replace(&quot;ref&quot;, &quot;citation&quot;);

  tippy(a, {
  	  content: document.querySelector(citationPath).textContent, placement: &quot;bottom&quot;,
  	  arrow: true
  })

  if (id in alreadySeenRefs) return;

  var p = a.closest(&quot;p&quot;);
  var currentId = p.getAttribute(&quot;id&quot;);

  if (currentId == null) {
    p.setAttribute(&quot;id&quot;, id)
  } else {
    document.querySelector(citationPath + &quot; a.ref-backlink&quot;).setAttribute(&quot;href&quot;, &quot;#&quot; + currentId)
  }

  alreadySeenRefs[id] = true;
  a.removeAttribute(&quot;id&quot;)
})
&lt;/script&gt;

</description>
        <pubDate>Tue, 17 Jul 2018 00:00:00 +0200</pubDate>
        <link>http://localhost:4000/estimators/</link>
        <guid isPermaLink="true">http://localhost:4000/estimators/</guid>
        
        
        <category>machine-learning,</category>
        
        <category>optimization</category>
        
      </item>
    
      <item>
        <title>Federated Learning</title>
        <description>&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;span id=&quot;ref-1&quot;&gt;&lt;/span&gt;In the past few years, machine learning has led to major breakthroughs in various areas, such as natural language processing, computer vision and speech recognition &lt;a href=&quot;#citation-1&quot; id=&quot;ref-1&quot; class=&quot;ref-link&quot;&gt;[1]&lt;/a&gt;
.
Much of this success has been based on collecting huge amounts of data. For example, one of Facebook’s latest &lt;a href=&quot;https://github.com/facebookresearch/Detectron&quot;&gt;Detectron&lt;/a&gt; models for object detection was trained on &lt;a href=&quot;https://code.facebook.com/posts/1700437286678763/advancing-state-of-the-art-image-recognition-with-deep-learning-on-hashtags/&quot;&gt;3.5 billion images&lt;/a&gt; from Instagram.&lt;/p&gt;

&lt;p&gt;For some applications of machine learning, this need of collecting data can be incredibly privacy-invasive.
One such example application is predicting the next word that a person is going to use by considering the previous words.
This is typically done using machine learning nowadays, e.g. with recurrent neural networks and LSTMs &lt;a href=&quot;#citation-2&quot; id=&quot;ref-2&quot; class=&quot;ref-link&quot;&gt;[2]&lt;/a&gt;
.
Although it is possible to train such a model using a text corpus from Wikipedia, the language found there differs from the one typically used by people in daily life.&lt;/p&gt;

&lt;p&gt;One potential use case for such a model is to improve the results of speech recognition, another one to predict the next word that is typed on a mobile phone to help people type more quickly.
In both cases, it would be beneficial to directly train on that data instead of using text from Wikipedia.
This would allow training a model on the same data distribution that is also used for making predictions.
However, directly collecting this data is a terrible idea because it is extremely private.
Users do not want to send everything they type to a server.&lt;/p&gt;

&lt;p&gt;Sending only randomized versions of the original data to the server, based on the ideas of &lt;a href=&quot;/differential-privacy/&quot;&gt;Differential Privacy&lt;/a&gt;, is one potential solution to this problem.
The second solution is &lt;a href=&quot;https://research.googleblog.com/2017/04/federated-learning-collaborative.html&quot;&gt;&lt;em&gt;Federated Learning&lt;/em&gt;&lt;/a&gt;, a new approach to machine learning where the training data does not leave the users’ computer at all.
Instead of sharing their data, users compute weight updates themselves using their locally available data.
It is a way of training a model without directly inspecting users’ data on a server.
This blog post gives a high-level introduction to Federated Learning and the challenges that arise in this problem setting.&lt;/p&gt;

&lt;h3 id=&quot;federated-optimization&quot;&gt;Federated Optimization&lt;/h3&gt;

&lt;p&gt;Federated Learning is a collaborative form of machine learning where the training process is distributed among many users.
A server has the role of coordinating everything but most of the work is not performed by a central entity anymore but by a &lt;em&gt;federation&lt;/em&gt; of users.&lt;/p&gt;

&lt;p&gt;Before the start of the actual training process, the server initializes the model.
Theoretically, this can be done arbitrarily, by using any of the common neural network initialization strategies or the equivalent for other model types.
In practice, it is a good idea to use publicly available data to pretrain the model.
For the example given above, this could be done by using text from Wikipedia.
Although this does not produce the best possible model, it is a good starting point and can reduce the time until the Federated Learning process converges.&lt;/p&gt;

&lt;p&gt;After the model is initialized, a certain number of users are randomly selected to improve the model.
Each sampled user receives the current model from the server and uses their locally
available data to compute a model update \(H_i\).
All these updates are sent back to the server where they are averaged, weighted by the number of training examples that the respective clients used.
The server then applies this update to the model, typically by using some form of &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;&gt;gradient descent&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/posts/federated-learning/iteration.png&quot; width=&quot;600&quot; style=&quot;margin: 25px auto; margin-top: 5px; display: block&quot; alt=&quot;One iteration of a federated learning system&quot; /&gt;&lt;/p&gt;

&lt;p&gt;All of this is called a communication round.
This process is then performed many times until the parameters of the model stabilize.
Ideally, this happens after as few communication rounds as possible.
To this end, it helps if the updates given by the users have a high quality.
For models that we train based on gradient descent, one useful approach is to take several steps of stochastic gradient descent locally on the user’s computer before sending the weight update back to the server &lt;a href=&quot;#citation-3&quot; id=&quot;ref-3&quot; class=&quot;ref-link&quot;&gt;[3]&lt;/a&gt;
.&lt;/p&gt;

&lt;p&gt;The hyperparameter that determines how many users are sampled in each communication round also influences how many rounds are required until convergence is reached.
However, at some point the average that is computed stabilizes and utilizing more users per round does not help to significantly reduce the number of communication rounds further.
Thus, it makes sense to only query a smaller number of users in each iteration.&lt;/p&gt;

&lt;h3 id=&quot;applications&quot;&gt;Applications&lt;/h3&gt;

&lt;p&gt;In principle, this idea can be applied to any model for which some notion of updates can be defined.
This naturally includes everything based on gradient descent, which most of the popular models nowadays are.
Linear regression, logistic regression, neural networks and linear support vector machines can all be used for Federated Learning by letting users compute gradients.&lt;/p&gt;

&lt;p&gt;There are other models that are not based on gradients but where it is possible to define updates.
For k-means clustering, updates could correspond to moving the cluster centers.
If users compute the position of the new centers based on their local data, the weighted average across all these results gives us the true new position.
Similar averages can be used with the &lt;a href=&quot;http://theory.stanford.edu/~tim/s15/l/l8.pdf&quot;&gt;power iteration method&lt;/a&gt; to implement a distributed version of PCA.
For some other models like decision trees, it can be much harder to think of a federated version that allows for continuous updates.&lt;/p&gt;

&lt;p&gt;In terms of data, Federated Learning is especially useful in situations where users generate and label data themselves implicitly.
This is the case for the application of trying to predict the next word.
While users type on their keyboards, the model tries to predict the next word.
As soon as the user typed the next word, a new data point is created and the true label (the last word) is determined.
The model can then automatically update itself without having to store the data permanently.
In such a situation, Federated Learning is extremely powerful because models can be trained with a huge amount of data that is not stored and not directly shared with a server at all.
We can thus make use of a lot of data that we could otherwise not have used without violating the users’ privacy.&lt;/p&gt;

&lt;h3 id=&quot;unique-characteristics&quot;&gt;Unique Characteristics&lt;/h3&gt;

&lt;p&gt;While Federated Learning might sound similar to distributed machine learning on a technical level, there are some major differences to applications in data centers where the training data is distributed among many machines &lt;a href=&quot;#citation-4&quot; id=&quot;ref-4&quot; class=&quot;ref-link&quot;&gt;[4]&lt;/a&gt;
.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Huge number of clients&lt;/strong&gt;: Since machine learning generally requires a lot of data, the applications that use it have to have many users. Every one of these users could theoretically participate in Federated Learning, making it far more distributed than anything in a data center&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Non-identical distributions&lt;/strong&gt;: In a data center setting, it is possible to ensure that every machine has a representative set of data so that all updates look very similar. In Federated Learning, this cannot be guaranteed. We have to expect that users generate data from completely different distributions, i.e. we cannot make &lt;a href=&quot;https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables&quot;&gt;iid&lt;/a&gt; assumptions. While similar users might have similar local training data, two randomly picked users could produce very different weight updates&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unbalanced number of samples&lt;/strong&gt;: Along the same lines, we cannot expect most users to have the same number of local training examples. There could be users with only a handful of data points, while others might have thousands&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Slow and unstable communication&lt;/strong&gt;: In a data center, it is expected that nodes can communicate comparatively quickly with each other and that it is ensured that messages do not get lost. In Federated Learning, these assumptions cannot be made. Uploads are typically going to be much &lt;a href=&quot;http://www.speedtest.net/reports/united-states/&quot;&gt;slower&lt;/a&gt; than downloads and, especially if the connection is from a cell phone, it might be extremely slow. Some clients might also currently not be connected to the internet and will not respond at all&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These properties motivate why Federated Learning requires its own specialized algorithms.&lt;/p&gt;

&lt;h3 id=&quot;compression&quot;&gt;Compression&lt;/h3&gt;

&lt;p&gt;Neural networks commonly have millions of parameters nowadays.
Sending updates for so many values to a server leads to huge communication costs with a growing number of users and iterations.
Thus, a naive approach to sharing weight updates is not feasible for larger models.
Since uploads are typically much slower than downloads, it is acceptable that users have to download the current model, while compression methods should be applied to the uploaded data.&lt;/p&gt;

&lt;p&gt;Of course, lossless compression techniques can be used and it might make sense to only send updates once a good network connection is possible.
Additionally, specialized compression techniques for Federated Learning can be applied &lt;a href=&quot;#citation-4&quot; id=&quot;ref-4&quot; class=&quot;ref-link&quot;&gt;[4]&lt;/a&gt;
.
Since only the average update is required to compute the next model, these compression methods try to encode updates with fewer bits while keeping the average stable.
It is acceptable that individual updates are compressed in a lossy manner, as long as the overall average does not change too much.&lt;/p&gt;

&lt;p&gt;On a high level, compression algorithms for Federated Learning can be put into two classes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Sketched updates&lt;/em&gt;: Clients compute a normal weight update and perform a compression afterwards. The compressed update is often an &lt;a href=&quot;/estimators/&quot;&gt;unbiased estimator&lt;/a&gt; of the true update, meaning they are the same on average. One of the more sophisticated such techniques is &lt;a href=&quot;/probabilistic-quantization/&quot;&gt;Probabilistic Quantization&lt;/a&gt;, which I described in more detail in another blog post&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Structured updates&lt;/em&gt;: During the optimization process, the update is restricted to be of a form that allows for an efficient compression. For example, the updates might be forced to be sparse or low-rank. The optimization then finds the best possible update of this form&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are no strong guarantees about which method works the best.
It heavily depends on the problem and the distributions of the updates.
Like in many parts of machine learning, different methods just have to be tested and are compared empirically.&lt;/p&gt;

&lt;h3 id=&quot;privacy&quot;&gt;Privacy&lt;/h3&gt;

&lt;p&gt;On a first look, Federated Learning seems like a method that it is very privacy-friendly.
However, one could think about an attacker that analyzes the weights to make conclusions about the data of users &lt;a href=&quot;#citation-5&quot; id=&quot;ref-5&quot; class=&quot;ref-link&quot;&gt;[5]&lt;/a&gt;
.
If the behavior of the coordinating server is also adversarial, the model could be a neural network with so much capacity that it overfits badly.
Since neural networks are &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_approximation_theorem&quot;&gt;universal function approximators&lt;/a&gt;, this model might just learn to approximate the function that directly acts as a look-up table to the data used for training.
In this case, the user’s data would not be private because it is still represented more or less clearly in the model.&lt;/p&gt;

&lt;p&gt;While this might sound unlikely if not done on purpose, there have been experiments that show it is possible to reconstruct some data points.
In one case, researchers were able to reconstruct images of faces that were used to train a face recognition model &lt;a href=&quot;#citation-6&quot; id=&quot;ref-6&quot; class=&quot;ref-link&quot;&gt;[6]&lt;/a&gt;
.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/differential-privacy/&quot;&gt;Differential Privacy&lt;/a&gt;, which I wrote about in another post, is one solution to this problem.
By formalizing what privacy means, we can analyze how well the learning algorithm respects privacy.
To employ this technique to Federated Learning, the notion of privacy is adapted to a user level: It should be very hard to tell whether a user contributed to the training of the model.
This is done using a stochastic framework.
By adding noise to update data shared by the user, the reports of individuals become much harder to analyze, while the noise can be estimated well for the aggregated data.&lt;/p&gt;

&lt;p&gt;Concretely, this involves several changes to the previous Federated Learning algorithm &lt;a href=&quot;#citation-6&quot; id=&quot;ref-6&quot; class=&quot;ref-link&quot;&gt;[6]&lt;/a&gt;
:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Users are randomly sampled with some probability instead of always sampling a fixed number of users. This is to ensure that users can still be sampled independently of each other. More sophisticated techniques than simple sampling should not be used because they might add a bias for certain users, which makes it more difficult to ensure their privacy&lt;/li&gt;
  &lt;li&gt;The updates that users send to the server need to have a bounded L2 norm. This limits how much individuals can influence the final weights. The motivation is that it should be prevented that individuals can be identified because they are the only ones who would propose large updates. In the case of neural networks, bounding the norm corresponds to &lt;a href=&quot;https://hackernoon.com/gradient-clipping-57f04f0adae&quot;&gt;gradient clipping&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Noise is added to the final update for the model, similar to most Differential Privacy algorithms&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In experiments, it has been shown that the same accuracy as before can be achieved with these changes &lt;a href=&quot;#citation-6&quot; id=&quot;ref-6&quot; class=&quot;ref-link&quot;&gt;[6]&lt;/a&gt;
.
However, the computational cost to get there is much higher.
In a real implementation, this could correspond to a slower convergence rate.&lt;/p&gt;

&lt;h3 id=&quot;encryption&quot;&gt;Encryption&lt;/h3&gt;

&lt;p&gt;Encryption for Federated Learning is a topic that is close to the privacy aspect previously discussed.
By using cryptography techniques, it is possible to ensure that the updates of individuals can only be read when enough users submitted updates &lt;a href=&quot;#citation-7&quot; id=&quot;ref-7&quot; class=&quot;ref-link&quot;&gt;[7]&lt;/a&gt;
.
This makes man-in-the-middle attacks much harder: An attacker cannot make conclusions about the training data based on the intercepted network activity of an individual user.
To be able to do that, they would need to intercept the messages of many users.&lt;/p&gt;

&lt;h3 id=&quot;personalization&quot;&gt;Personalization&lt;/h3&gt;

&lt;p&gt;A potential extension of Federated Learning could be customization.
While users help to train a central model, they also locally personalize it using their own data.
A simple implementation of this is a two-phase training process.
In the first step, a central model is collaboratively trained by all users.
After that, users locally adapt the model to their own preferences.&lt;/p&gt;

&lt;p&gt;This approach has an obvious drawback: Once users start personalizing the model, they cannot help to train the central one anymore.
That might be bad because there could be situations where the model becomes outdated.
A second approach is to personalize the input that the model receives.
Additionally to the actual input, the model also receives a personalized vector which encodes the preferences of the respective user.&lt;/p&gt;

&lt;p&gt;Once the model itself was trained to a sufficient quality, users start optimizing the personalized vector as well.
The centralized model is still improved periodically.
Over time, the centralized model keeps improving and adapting to changes, while users can also keep improving their personalization settings.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Current machine learning approaches require the availability of large datasets.
These are usually created by collecting huge amounts of data from users.
Federated Learning is a more flexible technique that allows training a model without directly seeing the data.
Although the learning algorithm is used in a distributed way, Federated Learning is very different to the way machine learning is used in data centers.
Many guarantees about distributions cannot be made and communication is often slow and unstable.&lt;/p&gt;

&lt;p&gt;To be able to perform Federated Learning efficiently, optimization algorithms can be adapted and various compression schemes can be used.
The privacy aspect can be tackled using Differential Privacy and encryption.
Since the system in general is quite flexible, it can be adapted to allow for locally personalized models.
Although there have been several papers about Federated Learning, it is still quite new and not many uses of it were reported by the industry yet.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ol class=&quot;references-list&quot;&gt;
	
	&lt;li&gt;&lt;span id=&quot;citation-1&quot;&gt;LeCun, Y., Bengio, Y. and Hinton, G., 2015. Deep learning. nature, 521(7553), p.436. &lt;a href=&quot;#ref-1&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-2&quot;&gt;Sundermeyer, M., Schlüter, R. and Ney, H., 2012. LSTM neural networks for language modeling. In Thirteenth Annual Conference of the International Speech Communication Association. &lt;a href=&quot;#ref-2&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-3&quot;&gt;McMahan, H.B., Moore, E., Ramage, D. and Hampson, S., 2016. Communication-efficient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629. &lt;a href=&quot;#ref-3&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-4&quot;&gt;Konečný, J., McMahan, H.B., Yu, F.X., Richtárik, P., Suresh, A.T. and Bacon, D., 2016. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492. &lt;a href=&quot;#ref-4&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-5&quot;&gt;McMahan, H.B., Ramage, D., Talwar, K. and Zhang, L., 2017. Learning differentially private language models without losing accuracy. arXiv preprint arXiv:1710.06963. &lt;a href=&quot;#ref-5&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-6&quot;&gt;Fredrikson, M., Jha, S. and Ristenpart, T., 2015, October. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (pp. 1322-1333). ACM. &lt;a href=&quot;#ref-6&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-7&quot;&gt;Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.B., Patel, S., Ramage, D., Segal, A. and Seth, K., 2017, October. Practical Secure Aggregation for Privacy-Preserving Machine Learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (pp. 1175-1191). ACM. &lt;a href=&quot;#ref-7&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;


&lt;/ol&gt;

&lt;script src=&quot;https://unpkg.com/tippy.js@3/dist/tippy.all.min.js&quot;&gt;&lt;/script&gt;

&lt;script&gt;
var alreadySeenRefs = {};

document.querySelectorAll(&quot;.ref-link&quot;).forEach(function (a) {
  var id = a.getAttribute(&quot;id&quot;);
  var citationPath = &quot;#&quot; + id.replace(&quot;ref&quot;, &quot;citation&quot;);

  tippy(a, {
  	  content: document.querySelector(citationPath).textContent, placement: &quot;bottom&quot;,
  	  arrow: true
  })

  if (id in alreadySeenRefs) return;

  var p = a.closest(&quot;p&quot;);
  var currentId = p.getAttribute(&quot;id&quot;);

  if (currentId == null) {
    p.setAttribute(&quot;id&quot;, id)
  } else {
    document.querySelector(citationPath + &quot; a.ref-backlink&quot;).setAttribute(&quot;href&quot;, &quot;#&quot; + currentId)
  }

  alreadySeenRefs[id] = true;
  a.removeAttribute(&quot;id&quot;)
})
&lt;/script&gt;

</description>
        <pubDate>Wed, 09 May 2018 09:54:00 +0200</pubDate>
        <link>http://localhost:4000/federated-learning/</link>
        <guid isPermaLink="true">http://localhost:4000/federated-learning/</guid>
        
        
        <category>machine-learning,</category>
        
        <category>federated-learning</category>
        
      </item>
    
      <item>
        <title>RProp</title>
        <description>&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;em&gt;RProp&lt;/em&gt; is a popular gradient descent algorithm that only uses the signs of
gradients to compute updates &lt;a href=&quot;#citation-1&quot; id=&quot;ref-1&quot; class=&quot;ref-link&quot;&gt;[1]&lt;/a&gt;
 &lt;a href=&quot;#citation-2&quot; id=&quot;ref-2&quot; class=&quot;ref-link&quot;&gt;[2]&lt;/a&gt;
. It stands for &lt;em&gt;Resilient Propagation&lt;/em&gt; and works
well in many situations because it adapts the step size dynamically for each
weight independently. This blog posts gives an introduction to RProp
and motivates its design choice of ignoring gradient magnitudes.&lt;/p&gt;

&lt;p&gt;Most gradient descent variants use the sign and the magnitude of the gradient.
The gradient points in the direction of steepest ascent.
Because we typically want to find a minimum, we follow the gradient in the
opposite direction.
This direction is completely determined by the sign of the gradient.&lt;/p&gt;

&lt;h3 id=&quot;gradient-magnitudes&quot;&gt;Gradient magnitudes&lt;/h3&gt;

&lt;p&gt;To decide on the step size, a scaled version of the gradient’s magnitude is
generally used by most gradient descent algorithms.
This heuristic often works well but there is no guarantee that it is
always a good choice.
To see that it can work extremely badly, and does not have
to contain valuable information, we consider a function \(f\).
The plots below show \(f\) as well as two scaled versions.&lt;/p&gt;

&lt;table class=&quot;image&quot;&gt;
	&lt;caption align=&quot;bottom&quot; style=&quot;&quot;&gt;Three functions with the same optima but vastly different gradients&lt;/caption&gt;
	&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/assets/posts/rprop/scales.png&quot; alt=&quot;Three functions with the same optima but vastly different gradients&quot; width=&quot;&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;All three of these functions have the exact same optima, so the step updates
using gradient descent should all be similar.
However, if we determine the step size using the gradient’s
magnitude, then the step sizes for the three functions differ by
orders of magnitude.
Even worse, the gradient virtually vanishes for the second function and explodes
for the third, as shown below:&lt;/p&gt;

&lt;table class=&quot;image&quot;&gt;
	&lt;caption align=&quot;bottom&quot; style=&quot;&quot;&gt;The first derivatives of the three functions &lt;/caption&gt;
	&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/assets/posts/rprop/scales-diff.png&quot; alt=&quot;The first derivatives of the three functions &quot; width=&quot;&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;This shows that the gradient’s magnitude does not necessarily contain useful
information for determining the step size.
Even though optima can still be found by choosing appropriate learning rates,
this makes it clear that using the gradient’s magnitude at all is sometimes questionable.
Using a fixed learning rate will also fail if only some parts of the function
are scaled.&lt;/p&gt;

&lt;h3 id=&quot;updating-weights&quot;&gt;Updating weights&lt;/h3&gt;

&lt;p&gt;Modern gradient descent variants try to circumvent this problem by dynamically
adapting the step size.
RProp does this in a way that only requires the sign of the gradient.
By ignoring the gradient’s magnitude, RProp has no problems if a function has a few very
steep areas.&lt;/p&gt;

&lt;p&gt;Concretely, RProp uses a different step size for each dimension.
Let \(\eta_i^{(t)}\) be the step size for the \(i\)-th weight in the \(t\)-th
iteration of gradient descent.
The value for the first and second iteration, \(\eta_i^{(0)}\) and
\(\eta_i^{(1)}\), is a hyperparameter that needs to be chosen in advance.
This step size is then dynamically adapted for each weight, depending on the gradient.&lt;/p&gt;

&lt;p&gt;The weights themselves are updated using&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i^{(t)} = w_i^{(t - 1)} - \eta_i^{(t - 1)} * \operatorname{sgn}\left(\frac{\partial E^{(t -
    1)}}{\partial w_i^{(t - 1)}}\right)&lt;/script&gt;

&lt;p&gt;where the sign of the partial derivative of the error in the last step
with respect to the respective weight is computed.
We go in the direction of descent using the determined step size.&lt;/p&gt;

&lt;h3 id=&quot;adapting-the-step-size&quot;&gt;Adapting the step size&lt;/h3&gt;

&lt;p&gt;In each iteration of RProp, the gradients are computed and the step sizes are
updated for each dimension individually.
This is done by comparing the gradient’s sign of the current and previous
iteration.
The idea here is the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When the signs are the same, we go in the same direction as in the
  previous iteration. Since this seems to be a good direction, the step size
  should be increased to go to the optimum more quickly&lt;/li&gt;
  &lt;li&gt;If the sign changed, the new update is moving in a different direction.
  This means that we just jumped over an optimum.
  The step size should be decreased to avoid jumping over the optimum again&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A visualization of this idea is shown below.&lt;/p&gt;

&lt;table class=&quot;image&quot;&gt;
	&lt;caption align=&quot;bottom&quot; style=&quot;&quot;&gt;The gradient direction changes when jumping over optima&lt;/caption&gt;
	&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/assets/posts/rprop/jumps.png&quot; alt=&quot;The gradient direction changes when jumping over optima&quot; width=&quot;&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;To implement this update scheme, the following formula is used:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\eta_i^{(t)} = \begin{cases}
    \min(\eta_i^{(t - 1)} * \alpha, \eta_{\max}) &amp; \text{if } \frac{\partial E^{(t)}}{\partial w_i^{(t)}} * \frac{\partial E^{(t - 1)}}{\partial w_i^{(t - 1)}} &gt; 0 \\
    \max(\eta_i^{(t - 1)} * \beta, \eta_{\min}) &amp; \text{if } \frac{\partial E^{(t)}}{\partial w_i^{(t)}} * \frac{\partial E^{(t - 1)}}{\partial w_i^{(t - 1)}} &lt; 0 \\
    \eta_i^{(t - 1)} &amp; \text{otherwise}
    \end{cases}
\label{eq:rprop} %]]&gt;&lt;/script&gt;

&lt;p&gt;where \(\alpha &amp;gt; 1 &amp;gt; \beta\) scale the step size, depending on whether
the speed should be increased or decreased. The step size is then clipped using
\(\eta_{\min}\) and \(\eta_{\max}\) to avoid it becoming too large or too small.
If a gradient was zero, a local optimum for this weight was found and the step
size is not changed.&lt;/p&gt;

&lt;h3 id=&quot;hyperparameters&quot;&gt;Hyperparameters&lt;/h3&gt;

&lt;p&gt;These seem like many hyperparameters to choose, but in practice there are known values for them that generally work well.
It is also not problematic if the clipping values \(\eta_{\min}\) and \(\eta_{\max}\) are respectively smaller and larger than necessary because an inconvenient step size is generally adapted quickly.&lt;/p&gt;

&lt;p&gt;Popular values for \(\alpha\) and \(\beta\) are \(1.2\) and \(0.5\).
Heuristically, it works well to increase the step size slowly, while allowing for the possibility of quickly decreasing it when jumping around an optimum.
For fine-tuning the weights, it is important that \(\beta\) is not the reciprocal of \(\alpha\), to allow for many different step sizes.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;One advantage of RProp that was not discussed so far is having a different step
size for each weight.
If one weight is already very close to its optimal value while a second weight
still needs to be changed a lot, this is not a problem for RProp.
Other gradient descent variants can have much more problems with such a
situation, especially because the gradient magnitudes can be misleading here.&lt;/p&gt;

&lt;p&gt;While RProp works well in a lot of situations, it is not perfect.
For instance, RProp generally requires large batch updates.
If there’s too much randomness in stochastic gradient descent, then the step sizes jump around too much
and the updates work badly.&lt;/p&gt;

&lt;p&gt;Implementing RProp is quite straightforward.
To get a better understanding of RProp, reading the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/master/torch/optim/rprop.py&quot;&gt;PyTorch
implementation&lt;/a&gt; can also be helpful.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ol class=&quot;references-list&quot;&gt;
	
	&lt;li&gt;&lt;span id=&quot;citation-1&quot;&gt;Rojas, R., 2013. Neural networks: a systematic introduction. Springer Science &amp;amp; Business Media. &lt;a href=&quot;#ref-1&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-2&quot;&gt;Riedmiller, M. and Braun, H., 1993. A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In Neural Networks, 1993., IEEE International Conference on (pp. 586-591). IEEE. &lt;a href=&quot;#ref-2&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;


&lt;/ol&gt;

&lt;script src=&quot;https://unpkg.com/tippy.js@3/dist/tippy.all.min.js&quot;&gt;&lt;/script&gt;

&lt;script&gt;
var alreadySeenRefs = {};

document.querySelectorAll(&quot;.ref-link&quot;).forEach(function (a) {
  var id = a.getAttribute(&quot;id&quot;);
  var citationPath = &quot;#&quot; + id.replace(&quot;ref&quot;, &quot;citation&quot;);

  tippy(a, {
  	  content: document.querySelector(citationPath).textContent, placement: &quot;bottom&quot;,
  	  arrow: true
  })

  if (id in alreadySeenRefs) return;

  var p = a.closest(&quot;p&quot;);
  var currentId = p.getAttribute(&quot;id&quot;);

  if (currentId == null) {
    p.setAttribute(&quot;id&quot;, id)
  } else {
    document.querySelector(citationPath + &quot; a.ref-backlink&quot;).setAttribute(&quot;href&quot;, &quot;#&quot; + currentId)
  }

  alreadySeenRefs[id] = true;
  a.removeAttribute(&quot;id&quot;)
})
&lt;/script&gt;

</description>
        <pubDate>Sun, 08 Apr 2018 12:54:00 +0200</pubDate>
        <link>http://localhost:4000/rprop/</link>
        <guid isPermaLink="true">http://localhost:4000/rprop/</guid>
        
        
        <category>machine-learning,</category>
        
        <category>optimization</category>
        
      </item>
    
      <item>
        <title>Probabilistic Quantization</title>
        <description>&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&quot;https://research.googleblog.com/2017/04/federated-learning-collaborative.html&quot;&gt;Federated Learning&lt;/a&gt;
is an exciting new subarea of machine learning where the
training process is distributed among many users &lt;a href=&quot;#citation-1&quot; id=&quot;ref-1&quot; class=&quot;ref-link&quot;&gt;[1]&lt;/a&gt;
. It is a form of collaborative
machine learning with the constraint that the communication can be slow and
unstable.&lt;/p&gt;

&lt;p&gt;This is easily worth its &lt;a href=&quot;/federated-learning/&quot;&gt;own post&lt;/a&gt; but in a nutshell Federated
Learning works like this: A central server maintains a machine learning model.
Training data is only available locally on the users’ devices, so from time to
time they get a copy of the model and improve it using their locally available
data. The weight updates are sent back to the server where they are averaged
and the model is updated. This process is then repeated from time to time.&lt;/p&gt;

&lt;p&gt;Federated Learning is an incredibly interesting topic because it allows users to
keep their data private while a high-quality model can still be trained using
it. There are, however, some challenges for making this work. One of them is
that a naive approach leads to extremely high communication costs. Neural networks
nowadays commonly have millions of parameters and sending updates for millions
of weights from a mobile device to a server is not really desirable.&lt;/p&gt;

&lt;h3 id=&quot;probabilistic-binarization&quot;&gt;Probabilistic Binarization&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Probabilistic binarization&lt;/em&gt;, or &lt;em&gt;quantization&lt;/em&gt; &lt;a href=&quot;#citation-2&quot; id=&quot;ref-2&quot; class=&quot;ref-link&quot;&gt;[2]&lt;/a&gt;
, is one solution to this problem. I found it to be
very elegant, so I decided it would be worth writing a blog post about.
Fundamentally, it removes a lot of information from individual updates which
allows us to encode them using much fewer bits. Still, by taking into account
the aggregated weight updates from a lot of users, not much information is lost.
This concept reminds me a lot of &lt;a href=&quot;/differential-privacy/&quot;&gt;Differential Privacy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To understand the idea, it is important to realize that only the average is
important. If a weight update \(h\) is randomized to a compressed version \(h’\),
then \(E[h’] = h\) needs to hold. In other words, the compressed estimate needs to
be correct on average. This is called an &lt;a href=&quot;/estimators/&quot;&gt;unbiased estimator&lt;/a&gt; in statistics.&lt;/p&gt;

&lt;p&gt;Having an unbiased estimator allows us to approximate the true weight update
more closely and closely as more weight updates are sent. This is known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Law_of_large_numbers&quot;&gt;law of
large numbers&lt;/a&gt;.
A binarization scheme that implements this idea looks as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
h' = \begin{cases}
		h_{\min} &amp; \text{with probability } (h_{\max} - h) / (h_{\max} - h_{\min})  \\
		h_{\max} &amp; \text{with probability } (h - h_{\min}) / (h_{\max} - h_{\min})
	\end{cases}
    \label{eq:probailistic-binarization} %]]&gt;&lt;/script&gt;

&lt;p&gt;where \(h_{\min}\) and \(h_{\max}\) are the smallest and largest values of the weight update.&lt;/p&gt;

&lt;p&gt;The denominator in the formula is a normalization factor to make sure that all
probabilities are between 0 and 1. The numerators compute the distances between
\(h\) and the two bounds. The bound that is closer to \(h\) is chosen with a
higher probability.&lt;/p&gt;

&lt;table class=&quot;image&quot;&gt;
	&lt;caption align=&quot;bottom&quot; style=&quot;&quot;&gt;A visualization of the probabilities in probabilistic binarization, assuming normalized distances&lt;/caption&gt;
	&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/assets/posts/probabilistic-quantization/distances.png&quot; alt=&quot;A visualization of the probabilities in probabilistic binarization, assuming normalized distances&quot; width=&quot;400&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The visualization above shows how the probabilities relate to \(h_{\min}\) and \(h_{\max}\).&lt;/p&gt;

&lt;h3 id=&quot;convergence&quot;&gt;Convergence&lt;/h3&gt;

&lt;p&gt;By using this randomization technique, the estimates converge slowly to the true
mean. I implemented some simulations to play with this. In the simulation shown below, 500
users sent random updates sampled from a normal distribution centered around 0.4 with
a standard deviation of 0.3. Each simulated user sent their true update and the
randomized one. The plot shows the average after \(x\) reports arrived at the
server.&lt;/p&gt;

&lt;table class=&quot;image&quot;&gt;
	&lt;caption align=&quot;bottom&quot; style=&quot;&quot;&gt;The average of compressed values stabilizes after around 200 users&lt;/caption&gt;
	&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;/assets/posts/probabilistic-quantization/law-of-large-numbers.png&quot; alt=&quot;The average of compressed values stabilizes after around 200 users&quot; width=&quot;400&quot; /&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;As shown, both mean estimates converge to 0.4. The randomized estimate has a slower convergence but it is worth noting that 500 users are not a lot when requiring data for machine learning. Still, the formal guarantee for convergence is a bit weaker since the training data of users is generally differently distributed.&lt;/p&gt;

&lt;p&gt;But all in all, this is a remarkable result: Just by querying some more users,
we got the same information while each client needed to send much fewer bits.
The compressed weight update \(h’\) can be encoded using one bit (1 for \(h_{\max}\), 0 for \(h_{\min}\))
whereas the original update \(h\) was a 32- or 64-bit float. This is a
huge compression factor.&lt;/p&gt;

&lt;p&gt;In Federated Learning, not all users are sampling updates from the same distribution.
However, we can assume that some users are going to generate similar updates.
By using the compression idea above, we can encode all updates with fewer bits and the hope is that the randomness averages out by querying enough users.&lt;/p&gt;

&lt;h3 id=&quot;probabilistic-quantization&quot;&gt;Probabilistic Quantization&lt;/h3&gt;

&lt;p&gt;The algorithm shown so far could be called &lt;em&gt;probabilistic binarization&lt;/em&gt;.
Quantization takes the idea one step further: Instead of sending one of two
possible values, several values are possible. To encode an update \(h\), the
probabilistic binarization scheme using the two closest values is applied.&lt;/p&gt;

&lt;p&gt;Of course, more bits are required to encode the possible values. The tradeoff is that convergence can be reached much more quickly and that the estimates for the
largest and smallest possible values may be less accurate. The resulting estimator still computes unbiased estimates.&lt;/p&gt;

&lt;p&gt;Finally, it is worth noting that there are further improvements to this algorithm &lt;a href=&quot;#citation-3&quot; id=&quot;ref-3&quot; class=&quot;ref-link&quot;&gt;[3]&lt;/a&gt;
.
By applying random rotations, the error when only a few users have sent their
data can be reduced. Additionally, this method can be combined with other
compression techniques for Federated Learning &lt;a href=&quot;#citation-2&quot; id=&quot;ref-2&quot; class=&quot;ref-link&quot;&gt;[2]&lt;/a&gt;
.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ol class=&quot;references-list&quot;&gt;
	
	&lt;li&gt;&lt;span id=&quot;citation-1&quot;&gt;McMahan, H.B., Moore, E., Ramage, D. and Hampson, S., 2016. Communication-efficient learning of deep networks from decentralized data. &lt;a href=&quot;#ref-1&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-2&quot;&gt;Konečný, J., McMahan, H.B., Yu, F.X., Richtárik, P., Suresh, A.T. and Bacon, D., 2016. Federated learning: Strategies for improving communication efficiency. &lt;a href=&quot;#ref-2&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;

	&lt;li&gt;&lt;span id=&quot;citation-3&quot;&gt;Suresh, A.T., Yu, F.X., McMahan, H.B. and Kumar, S., 2016. Distributed mean estimation with limited communication. &lt;a href=&quot;#ref-3&quot; class=&quot;ref-backlink&quot;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;


&lt;/ol&gt;

&lt;script src=&quot;https://unpkg.com/tippy.js@3/dist/tippy.all.min.js&quot;&gt;&lt;/script&gt;

&lt;script&gt;
var alreadySeenRefs = {};

document.querySelectorAll(&quot;.ref-link&quot;).forEach(function (a) {
  var id = a.getAttribute(&quot;id&quot;);
  var citationPath = &quot;#&quot; + id.replace(&quot;ref&quot;, &quot;citation&quot;);

  tippy(a, {
  	  content: document.querySelector(citationPath).textContent, placement: &quot;bottom&quot;,
  	  arrow: true
  })

  if (id in alreadySeenRefs) return;

  var p = a.closest(&quot;p&quot;);
  var currentId = p.getAttribute(&quot;id&quot;);

  if (currentId == null) {
    p.setAttribute(&quot;id&quot;, id)
  } else {
    document.querySelector(citationPath + &quot; a.ref-backlink&quot;).setAttribute(&quot;href&quot;, &quot;#&quot; + currentId)
  }

  alreadySeenRefs[id] = true;
  a.removeAttribute(&quot;id&quot;)
})
&lt;/script&gt;

</description>
        <pubDate>Sun, 25 Feb 2018 17:03:00 +0100</pubDate>
        <link>http://localhost:4000/probabilistic-quantization/</link>
        <guid isPermaLink="true">http://localhost:4000/probabilistic-quantization/</guid>
        
        
        <category>federated-learning</category>
        
      </item>
    
      <item>
        <title>What I read in 2017</title>
        <description>&lt;p&gt;Whenever I’m looking for a new book to read, I really enjoy looking at lists of
books other people read. Since I started reading more again this year, I thought
it’d be a good idea to write down some thoughts on the individual books.
I ended up picking up a new book pretty much every month. In the following, I
describe the more interesting ones of them.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;the-sense-of-style&quot;&gt;&lt;a href=&quot;https://www.amazon.com/Sense-Style-Thinking-Persons-Writing/dp/0143127799/r&quot;&gt;The Sense of Style&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/posts/books-2017/sense-of-style.jpg&quot; alt=&quot;The Sense of Style cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Sense of Style is a style guide that tries to explain how to write more elegantly and
effectively. I started reading this book at the very beginning of January, when I
was in the final weeks of working on my bachelor thesis. Of course, the timing was not a
coincidence, and I do feel like the book has helped to improve my thesis a bit.&lt;/p&gt;

&lt;p&gt;To my surprise however, I also ended up finding the book itself quite enjoyable to read. Partly, this
is definitely because it is written in a great style, by a highly eloquent author.
While I was reading the book, it also occurred to me that I never really thought
about the fact that there are style guides for writing prose, similar to how
programmers have style guides for code.&lt;/p&gt;

&lt;p&gt;The book addresses not only stylistic choices but also dives into more psychological
aspects. One of these points that I really internalized is what the author
calls the &lt;em&gt;curse of knowledge&lt;/em&gt;.
By having a deep understanding of a topic, it becomes incredibly hard to explain
it in a good way to people without much prior knowledge. This is due to the fact
that we forget the difficulties we had when first learning about
the topic ourselves. Now it all seems so simple and it’s easy to fall into the
trap of thinking that certain parts are also obvious to other people.&lt;/p&gt;

&lt;h3 id=&quot;the-signal-and-the-noise&quot;&gt;&lt;a href=&quot;https://www.amazon.com/Signal-Noise-Many-Predictions-Fail-but/dp/0143125087/&quot;&gt;The Signal and the Noise&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/posts/books-2017/signal-and-noise.jpg&quot; alt=&quot;The Signal and the Noise cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A lot of people will probably know the author, Nate Silver, for his incredibly
accurate election predictions. He is also well known for applying statistical
techniques to baseball, and for his articles at
&lt;a href=&quot;http://fivethirtyeight.com&quot;&gt;FiveThirtyEight&lt;/a&gt;, where he combines statistics with
journalism. I have to admit that this impressive biography is also what got me
interested in his book initially. While this is not the best reason to read a
book, I have to say that I really enjoyed it. Over the last year, I ended up recommending
the book to a lot of people.&lt;/p&gt;

&lt;p&gt;In each chapter, Silver explains how statistics are used to make
predictions in different fields.
The range of these applications is quite large and goes from elections over
weather reports and climate change to online poker.
Because there is such a large variety of topics, the book never gets boring. It’s also
interesting to see how complicated things like weather predictions are, while
they are so ordinary for us that we don’t give them much thought.&lt;/p&gt;

&lt;p&gt;When I started reading the book, my expectations were a bit off, as I expected a
lot more math. Instead, the book is more concerned with explaining the
fundamental ideas on a popular scientific level. This means that the book
contains nearly no math but often gets into psychological aspects. For example,
a good part of the chapter on weather predictions is spent on explaining why some
predictions are made more inaccurate for psychological reasons. This is not
necessarily a negative criticism of the book, as I also deeply enjoyed these topics.
Lastly, I want to mention that the book also contains a really good high-level introduction to
Bayesian statistics.&lt;/p&gt;

&lt;h3 id=&quot;reinforcement-learning-an-introduction&quot;&gt;&lt;a href=&quot;http://incompleteideas.net/sutton/book/the-book.html&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/posts/books-2017/rl.jpg&quot; alt=&quot;Reinforcement Learning: An Introduction cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Reinforcement learning is a subarea of Machine Learning with some essential
differences to (un)supervised learning. The fundamental idea is that we want to
train an agent that learns how to perform tasks in a dynamic environment. In
contrast to supervised learning, the feedback given to the agent is delayed and
does not tell it what the best action would have been.&lt;/p&gt;

&lt;p&gt;Because reinforcement learning is a bit of a niche area in Machine Learning, I
never really got into it when I was new to the field. Still, the fundamental
idea sounded incredibly interesting to me, so in the holidays after the winter
semester I decided to work through this book. This was just in time to read the
reworked second edition.&lt;/p&gt;

&lt;p&gt;The book was written by Sutton and Barto who are renowned experts in the field
and came up with many of the most important ideas themselves. While the book
does contain a large amount of math, it’s still very accessible because
everything is explained up from the ground up.&lt;/p&gt;

&lt;p&gt;Working through the book was a lot of fun and I ended up &lt;a href=&quot;https://github.com/florian/reinforcement-learning&quot;&gt;implementing&lt;/a&gt; many algorithms from the book using Jupyter notebooks.
By now, reinforcement learning is also one of my absolute favorite areas of
machine learning, and the book definitely contributed to this. I still marvel at
the way it builds up the entire framework step by step.&lt;/p&gt;

&lt;h3 id=&quot;the-seven-habits-of-highly-effective-people&quot;&gt;&lt;a href=&quot;https://www.amazon.com/Habits-Highly-Effective-People-Powerful/dp/1451639619/&quot;&gt;The Seven Habits of Highly Effective People&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/posts/books-2017/effective.jpg&quot; alt=&quot;The Seven Habits of Highly Effective People cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This book tries to explain what traits most successful people have in common.
The fundamental idea behind the book is that there are seven such habits that
most of these people share.
One chapter is devoted to each habit, in which it is discussed in a lot of detail.&lt;/p&gt;

&lt;p&gt;On the first few pages of the book, there is an incredible number of quotes from
well-known people that praise the book. This increased my expectations even
further, but ultimately I really did not like the book. The habits themselves all make
sense but they are discussed in a huge length, which becomes off-putting at some
point. Other advice from the book is way too over-engineered and complicated. I
feel like it would’ve made more sense to write down these ideas in a long essay
rather than in a 400-page book.&lt;/p&gt;

&lt;h3 id=&quot;a-clash-of-kings&quot;&gt;&lt;a href=&quot;https://www.amazon.com/Clash-Kings-Song-Fire-Book/dp/0553579908&quot;&gt;A Clash of Kings&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/posts/books-2017/acok.jpg&quot; alt=&quot;A Clash of Kings cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A Clash of Kings&lt;/em&gt; is the second book from the &lt;em&gt;A Song of Ice and Fire&lt;/em&gt; series
that &lt;em&gt;Game of Thrones&lt;/em&gt; is based on. After I read the first book in 2016, I was
planning to read the other four books this year just so that I would be done
when the sixth book was going to come out. Unfortunately, that book got delayed further
and further and is not even out by now, so I decided to pause again after the second book.&lt;/p&gt;

&lt;p&gt;All in all, I really enjoyed the book. Knowing the important plot points from
the TV series takes some fun away, but the second book already starts to deviate
from the show, so there were still some surprises left. The writing style is
also great and I really like how each chapter is told from the point of view of
a different character.&lt;/p&gt;

&lt;h3 id=&quot;a-mathematicians-apology&quot;&gt;&lt;a href=&quot;https://www.math.ualberta.ca/mss/misc/A%20Mathematician%27s%20Apology.pdf&quot;&gt;A Mathematician’s Apology&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/posts/books-2017/apology.jpg&quot; alt=&quot;A Mathematician's Apology&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hardy was one of the most brilliant mathematicians of the 20th century. &lt;em&gt;A
Mathematician’s Apology&lt;/em&gt; is an essay from him where he tries to reason about why
it is worth spending time on mathematics. The word &lt;em&gt;apology&lt;/em&gt; refers to a
justification here.&lt;/p&gt;

&lt;p&gt;At the core, it is Hardy’s belief that the most beautiful kind of math is not
pursued for the sake of possible applications but just for the mathematics
themselves. Interestingly, he argues that number theory, his field of work, is
such an example. By now, there are of course many important applications for
number theory. Hardy compares pure mathematics to fields like poetry that are just
pursued because there is an aesthetic in them.&lt;/p&gt;

&lt;p&gt;All in all, I found the essay very interesting to read. Hearing about his point
of view made me change my perspective on certain areas of mathematics a little
bit. However, I do not agree on all his points, some of his views just seem too
excessive.&lt;/p&gt;

&lt;h3 id=&quot;how-to-win-friends-and-influence-people&quot;&gt;&lt;a href=&quot;https://www.amazon.com/How-Win-Friends-Influence-People/dp/0671027034/&quot;&gt;How To Win Friends And Influence People&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/posts/books-2017/carnegie.jpg&quot; alt=&quot;How To Win Friends And Influence People&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;How To Win Friends And Influence People&lt;/em&gt; is one of the best-selling books of
all time and can be found on many book suggestion lists. Because I heard so much
about it, I intended to read this book for quite some time. In the book,
Carnegie describes fundamental social principles.&lt;/p&gt;

&lt;p&gt;Many of these rules are fairly obvious. However, some of them are definitely
worth reading about. For example, I found the parts about criticism and how
people deal with it interesting to read about from a psychological perspective.&lt;/p&gt;

&lt;h3 id=&quot;openintro-statistics&quot;&gt;&lt;a href=&quot;https://www.openintro.org/stat/&quot;&gt;OpenIntro Statistics&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/posts/books-2017/openintro-stats.jpg&quot; alt=&quot;OpenIntro Statistics cover&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After having focused my studies on Machine Learning, I felt like I learned a lot
about certain areas of statistics while not touching others at all. At some
point, I decided that I should spend a little bit of time learning the
fundamentals of statistics to be able to get a good overview of the entire
field. Especially significance tests were a part of statistics where I felt like
I really lacked knowledge.&lt;/p&gt;

&lt;p&gt;OpenIntro Statistics is a freely available textbook that seemed suitable for
this. It makes nearly no assumptions about the prior knowledge of the reader. On
the one hand, this is great. You can pick up the book without revising
any other topics beforehand. But it also means that some parts of the book are
incredibly basic.&lt;/p&gt;

&lt;p&gt;This is the first book where I actively started writing
&lt;a href=&quot;https://github.com/florian/reading-notes/blob/master/1_OpenIntro-Statistics.md&quot;&gt;notes&lt;/a&gt; that
summarize the most important points and concepts. This turned out to be incredibly effective,
and I still can’t believe I never did this in an organized fashion before.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Looking back, I really like that the books cover a large number of different
genres. I also only just noticed that I only read books in English this year.
I’m pretty sure it was the same for 2016, which means I haven’t read a book in
German for at least two years.&lt;/p&gt;
</description>
        <pubDate>Sat, 30 Dec 2017 13:00:00 +0100</pubDate>
        <link>http://localhost:4000/reading-2017/</link>
        <guid isPermaLink="true">http://localhost:4000/reading-2017/</guid>
        
        
        <category>reading-notes</category>
        
      </item>
    
      <item>
        <title>Add-on recommendations for Firefox users</title>
        <description>&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; url=https://medium.com/firefox-context-graph/add-on-recommendations-for-firefox-users-7774cc5a5117&quot; /&gt;

&lt;style&gt; * { display: none } &lt;/style&gt;

&lt;p&gt;This is a reading suggestion for a very interesting post from the Mozilla Context Graph blog. The
blog post describes the new prototype of the Firefox add-on recommender
system: &lt;a href=&quot;https://medium.com/firefox-context-graph/add-on-recommendations-for-firefox-users-7774cc5a5117&quot;&gt;https://medium.com/firefox-context-graph/add-on-recommendations-for-firefox-users-7774cc5a5117&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It was also cross-posted to &lt;a href=&quot;https://www.a2p.it/wordpress/tech-stuff/mozilla/add-on-recommendations-for-firefox-users-a-prototype-recommender-system-leveraging-existing-data-sources/&quot;&gt;Alessio’s blog&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Dec 2017 00:23:10 +0100</pubDate>
        <link>http://localhost:4000/addon-recommender/</link>
        <guid isPermaLink="true">http://localhost:4000/addon-recommender/</guid>
        
        
        <category>differential-privacy</category>
        
      </item>
    
      <item>
        <title>Differential Privacy</title>
        <description>&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;span style=&quot;opacity:.5&quot;&gt;[This post was jointly written with &lt;a href=&quot;http://github.com/alexrs/&quot;&gt;Alejandro&lt;/a&gt; and also cross-posted on &lt;a href=&quot;https://www.alexrs.me/2017/rappor&quot;&gt;his blog&lt;/a&gt;. He wrote the first half, while I contributed the second.]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In 2007 Netflix offered a $1 million prize for a 10% improvement in its recommendation system. They also released a training
dataset for the competing developers to train their systems. In order to protect their customer’s privacy,
&lt;a href=&quot;https://www.cs.utexas.edu/~shmat/shmat_oak08netflix.pdf&quot;&gt;they removed personal information and replaced IDs with random IDs&lt;/a&gt;.
But Netflix is not the only movie-rating portal out there, there are many others such as IMDb. Researchers linked the
Netflix dataset with IMDb to de-anonymize the Netflix dataset using the dates on which a user rated certain movies.
This problem isn’t new and remains an important one as today, thanks to computers, we can access larger amounts of data and process them more easily.&lt;/p&gt;

&lt;p&gt;In the mid-90s, The Massachusetts Group Insurance Commission (GIC) released anonymized data on state employees that
showed every hospital visit. The goal was to help researchers, and the state spent time removing all obvious identifiers
such as name, address and Social Security number. A graduate student started hunting for the Governor’s hospital records
in the GIC data. She knew that Governor Weld resided in Cambridge, Massachusetts, a city of 54,000 residents and seven ZIP
codes. For twenty dollars, she purchased the complete voter rolls from the city of Cambridge. This is a database
containing, among other things, the name, address, ZIP code, birth date, and sex of every voter. By combining this data
with the GIC records, she found Governor Weld with ease. Only six people in Cambridge shared his birth date, only three
of them were men, and of them, only he lived in his ZIP code.
&lt;a href=&quot;https://fpf.org/wp-content/uploads/The-Re-identification-of-Governor-Welds-Medical-Information-Daniel-Barth-Jones.pdf&quot;&gt;The Governor’s health records were de-anonymized.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So, how can we solve this problem? Personal data is already removed from the dataset, and it’s impossible to know whether
a dataset can be used to de-anonymize another one. Here is where &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;&gt;Differential Privacy&lt;/a&gt; appears.&lt;/p&gt;

&lt;p&gt;It formalizes the idea that a query should not reveal whether anyone is present in a dataset, much less what their
data are. This field was defined by &lt;a href=&quot;https://en.wikipedia.org/wiki/Cynthia_Dwork&quot;&gt;Cynthia Dwork&lt;/a&gt; In 2006, using work that
started appearing in 2003. It is based on the ideas of &lt;a href=&quot;https://en.wikipedia.org/wiki/Randomized_response&quot;&gt;randomized response&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;randomized-response&quot;&gt;Randomized response&lt;/h2&gt;
&lt;p&gt;Let’s imagine you’re asked “Do you own the attribute A?”, but you don’t want to answer directly. You can use this procedure:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Throw a coin.&lt;/li&gt;
  &lt;li&gt;If head, then answer honestly.&lt;/li&gt;
  &lt;li&gt;If tail, then throw the coin again and answer “Yes” if head, “No” if tail.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;
If the attribute \( A \) is synonymous with illegal behavior, then answering “Yes” is not incriminating.&lt;/p&gt;

&lt;p&gt;Many responses are significant. Positive responses are given to \( 1/4 \) by people who don’t have the attribute A and \( 3/4 \)
by people who possess it.&lt;/p&gt;

&lt;p&gt;Then we expect to obtain \( (1/4)(1-p) + (3/4)p = (1/4) + p/2 \) positive responses. Hence is possible to estimate p.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you’re interested in understanding more on how Differential Privacy works, &lt;a href=&quot;https://robertovitillo.com/2016/07/29/differential-privacy-for-dummies/&quot;&gt;here&lt;/a&gt; you can find more information.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Now, the question is, how can we use this technique to collect more complex data?&lt;/p&gt;

&lt;h2 id=&quot;rappor&quot;&gt;RAPPOR&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42852.pdf&quot;&gt;RAPPOR&lt;/a&gt; is an algorithm developed
by Google whose main purpose is to collect data while adding random noise to guarantee Differential Privacy.&lt;/p&gt;

&lt;p&gt;Each user is assigned to one of \( m\) cohorts. The value to encode is passed through \( h\) hash functions to encode it into a Bloom filter, and noise is added with probabilities \( p, q, f \). Bloom Filters were also described in more detail on &lt;a href=&quot;/bloom-filters/&quot;&gt;this blog before&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In RAPPOR, we need to set different parameters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Size of the Bloom filter, \( k \)&lt;/strong&gt;: RAPPOR uses a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bloom_filter&quot;&gt;Bloom filter&lt;/a&gt;
to report the data. When selecting the bloom filter size we should have in mind how many unique values are expected.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of hash functions, \( h \)&lt;/strong&gt;: Bloom filters uses hash functions to encode the values.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Number of cohorts, \( m \)&lt;/strong&gt;: To avoid collisions, RAPPOR divides the population into different cohorts.
This value must be chosen carefully. If it’s too small, collisions are still quite likely, while if it’s too large then
each individual cohort provides an insufficient signal due to its small sample size.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Probabilities \( p, q, f \)&lt;/strong&gt;: Noise is added to the Bloom filter with different probabilities. These
probabilities determine the level of Differential Privacy along with the number of hash functions used.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;em&gt;Note: This is a very simplified version of RAPPOR, you shouldn’t use it in real life. For optimized implementations, &lt;a href=&quot;https://github.com/google/rappor/&quot;&gt;look at the original Google’s repository&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The first step is to encode the original value into the Bloom filter using \( h \) hash functions.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cohort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_hashes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_bloombits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# create the bloom filter&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'0'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_bloombits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# create the hash function to use&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;md5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hashlib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;md5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cohort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;digest&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;digest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# get the indexes for encode the original value into the  bloom filter&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;ord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;digest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_bloombits&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_hashes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# set the corresponding 'bits' to 1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'1'&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This Bloom filter never leaves the client. The next step is known as Permanent Randomized Response. Here the bits of the Bloom filter are set to 0 or 1 with probability \( f/2 \), or remains unchanged with probability \( 1 - f \). The resulting Bloom filter should be stored in the client and used in the future if the client needs to report the same value more than once.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_prr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SystemRandom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'1'&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'0'&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Instantaneous Randomize Response is computed using the probabilities \( p \) and \( q \). The resulting Bloom filter will have the bit in position \( i \) set to 1 with probability \( q \) if its value was 1 in the PRR, or with probability \( p \) if its value was 0. The resulting bloom filter is sent for analysis.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_irr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SystemRandom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_p&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'0'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_q&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'1'&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bloom&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;hashlib&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SystemRandom&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# params&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_bloombits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;# Number of bloom filter bits (k)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_hashes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;          &lt;span class=&quot;c&quot;&gt;# Number of bloom filter hashes (h)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_cohorts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;        &lt;span class=&quot;c&quot;&gt;# Number of cohorts (m)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prob_p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.50&lt;/span&gt;           &lt;span class=&quot;c&quot;&gt;# Probability p&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prob_q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;           &lt;span class=&quot;c&quot;&gt;# Probability q&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prob_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.50&lt;/span&gt;           &lt;span class=&quot;c&quot;&gt;# Probability f&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# original value&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;v10&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# select cohort&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SystemRandom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cohort&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_cohorts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# encode&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;original&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cohort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_hashes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_bloombits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# prr&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;prr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_prr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;original&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# irr&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;irr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_irr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prob_q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;irr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/posts/differential-privacy/rappor.png&quot; alt=&quot;rappor&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;rappor-analysis&quot;&gt;RAPPOR analysis&lt;/h1&gt;

&lt;p&gt;After clients have generated their randomized responses, they send them to a server.
This server has the task of aggregating the reports and figuring out which answers were actually given, and how often.
To do this, we make use of statistical techniques that are explained in the remainder of this blog post.&lt;/p&gt;

&lt;p&gt;The bit arrays are the only information we get from the clients. However, because of the Bloom filter, there are generally
infinitely many answers that lead to the same bits being set. This means that we need some set of answers that we explicitly
check for. We call this the &lt;em&gt;candidate set&lt;/em&gt;. What values are used as candidates is completely dependent on the data that we’re collecting.&lt;/p&gt;

&lt;p&gt;Of course, we know what bits would be set when hashing these candidate values.
If we would also know how often each bit was truly set in the original Bloom filters, before noise was added, then we
could model this problem using an equation system. In this system, we’re looking for candidate counts so that the bits set
equal the true number of times the individual bits were set. In statistics, this corresponds to a &lt;em&gt;regression&lt;/em&gt; problem.&lt;/p&gt;

&lt;h2 id=&quot;estimating-the-counts-of-bits&quot;&gt;Estimating the counts of bits&lt;/h2&gt;

&lt;p&gt;Of course, the whole point of differential privacy is that we don’t have access to the originally set bits, so we can’t
directly solve this hypothetical equation system. But what we can do is figure out estimates for how often the bits were set.
This is possible because we can estimate how much noise was added on average. I won’t go into detail for the exact formulas,
as they help little to build intuition, but they can be found in the original &lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42852.pdf&quot;&gt;RAPPOR paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While this approach of making estimates might sound a bit messy, it actually has a fairly good theoretical backing.
By the &lt;a href=&quot;https://en.wikipedia.org/wiki/Law_of_large_numbers&quot;&gt;&lt;em&gt;law of large numbers&lt;/em&gt;&lt;/a&gt;, the estimate will converge to the true counts with an increasing amount of data.
This also explains one very important constraint when using differential privacy: We need a lot of users to make sense of
the data. The estimates will be very accurate with many users. On the other hand, we can’t control for the random noise
well enough if we don’t have a good amount of data.&lt;/p&gt;

&lt;p&gt;After having estimated how often bits were changed for the randomized response, we can compute estimates for how often
the bits were set in the original Bloom filter. We’ll call these estimates our &lt;em&gt;target vector&lt;/em&gt; \( y \). Note that
this only gives us information about how often bits were set in total, across all users. We have absolutely no clue
about which users had the bits set in their original Bloom filter.&lt;/p&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;p&gt;To give a more concrete idea of where we’re going with this, we can stop for a moment and consider this simple example.
Let’s say we use a Bloom filter with three bits and two hash functions. After having received the randomized reports from
enough users, we estimate the following true bit counts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;bit 1: 3000&lt;/li&gt;
  &lt;li&gt;bit 2: 4000&lt;/li&gt;
  &lt;li&gt;bit 3: 1000&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re collecting data where each client can give exactly one answer out of the possible answers &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt; and &lt;em&gt;c&lt;/em&gt;.
These values correspond to our candidate set. When hashing the candidate values, the following bits would be set:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;a&lt;/em&gt;: bits 1, 2 would be set&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;b&lt;/em&gt;: bits 1, 3 would be set&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;c&lt;/em&gt;: bits 2, 3 would be set&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given this information, we are looking for counts of how often the answers &lt;em&gt;a&lt;/em&gt;, &lt;em&gt;b&lt;/em&gt; or &lt;em&gt;c&lt;/em&gt; were given so that we arrive at the
estimated numbers for the individual bits. The important inside here is that this is an equation system:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;bit 1: count_a + count_b = 3000&lt;/li&gt;
  &lt;li&gt;bit 2: count_a + count_c = 4000&lt;/li&gt;
  &lt;li&gt;bit 3: count_b + count_c = 1000&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note how this is not just any kind of equation system, it’s a &lt;em&gt;linear equation system&lt;/em&gt;.
This is great as there are many well-known ways to solve linear equation systems.
The straightforward solution for this specific system is that answer &lt;em&gt;a&lt;/em&gt; was given 3000 times, &lt;em&gt;b&lt;/em&gt; was never given, while &lt;em&gt;c&lt;/em&gt;
was given 1000 times.&lt;/p&gt;

&lt;p&gt;Of course, this is a very simple and artificially constructed example, it’s just meant to showcase the problem that the RAPPOR
analysis is being reduced to.&lt;/p&gt;

&lt;h2 id=&quot;creating-the-data-matrix-x&quot;&gt;Creating the data matrix X&lt;/h2&gt;

&lt;p&gt;Linear equation systems can generally be well presented using matrices and vectors. We already described out target vector
\( y \) earlier. What’s left to talk about is the data matrix \( X \). This matrix encodes what bits are set when candidate
values are hashed in different cohorts.&lt;/p&gt;

&lt;p&gt;The general idea here is that for each bit and cohort we add a row to the matrix. For each candidate value, we add a column.
A cell then has value 1 if the corresponding bit would be set when hashing the corresponding candidate value in the
respective cohort. Otherwise, it has value 0.&lt;/p&gt;

&lt;p&gt;In the above simple example, where we have no cohorts, \( X \) would look like this:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
X =
  \begin{bmatrix}
    1 &amp; 1 &amp; 0 \\
    1 &amp; 0 &amp; 1 \\
    0 &amp; 1 &amp; 1
  \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now, our linear equation system can be described by \( Xb = y \) where \( b \) gives us the candidate counts that explain the set bits.&lt;/p&gt;

&lt;h2 id=&quot;linear-regression&quot;&gt;Linear regression&lt;/h2&gt;

&lt;p&gt;All of this is a little bit too simplified. Usually, we can’t directly solve this equation system.
One reason is that \( y \) only contains estimates and that our candidate set might be incomplete.
This means that the equation system might not have a perfect solution and that we’re generally only looking for an approximate one.
However, this is still a fairly standard problem in statistics and is usually
solved by fitting a linear regression model.&lt;/p&gt;

&lt;p&gt;The other problem is that our system does not entirely consist of linear equations.
It wouldn’t make sense to have negative counts.
Thus, \( b \) may only contain nonnegative values.
This makes the problem a fair bit harder to solve, but again it’s not a
completely new problem.
There are some implementations of nonnegative least squares (nnls) solvers
available that allow us to find the best approximate solution to a linear equation system with the nonnegativity constraint.&lt;/p&gt;

&lt;h2 id=&quot;significance-tests&quot;&gt;Significance tests&lt;/h2&gt;

&lt;p&gt;It’s worth keeping in mind that we’re only operating on estimates and that some hash collisions are possible. There are
many different approximations for the linear equation system and it’s not clear whether candidates with very small counts
were actually reported in the original Bloom filters.&lt;/p&gt;

&lt;p&gt;All of this screams for statistical significance tests. Computing p-values for linear regression coefficients is a standard
practice and is usually done using t-tests. In our case, we use one-sided t-tests because the nonnegativity constraint means
that extreme results are only possible in one direction.&lt;/p&gt;

&lt;p&gt;We use a significance level of 0.05 to filter out candidate values that don’t have enough evidence for their associated counts.
Because there might be a lot of candidate values, we use a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bonferroni_correction&quot;&gt;Bonferroni corrected&lt;/a&gt; significance level.
Finally, only the candidate values and frequencies that we have enough confidence in are reported.&lt;/p&gt;

&lt;h2 id=&quot;evaluating-the-results&quot;&gt;Evaluating the results&lt;/h2&gt;

&lt;p&gt;To evaluate how well this works, we can perform a simple simulation. In the simulation shown here, clients report exactly one answer that’s chosen from an exponential distribution.
We can plot this distribution using a bar plot, where the index of possible answers is on the x-axis and their counts are on the y-axis.
&lt;img src=&quot;../../assets/posts/differential-privacy/original-distribution.png&quot; alt=&quot;Original distribution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The randomization technique is then applied and the randomized reports are analyzed by our algorithm. The distribution
generated by the algorithm can then be plotted on top of the previous image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/posts/differential-privacy/reported-distribution.png&quot; alt=&quot;Reported distribution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This simulation uses the same parameters that we plan to use in production. As one can see, it works quite well for this
original distribution. For the common candidate values, the reported counts are very close to the actual ones. However,
it’s not possible to detect candidate values that only occurred very few times. There’s just not enough evidence to report
them with confidence. Of course only being able to detect common values is not necessarily a bad thing as it means more
privacy for users that gave unusual answers.&lt;/p&gt;
</description>
        <pubDate>Thu, 12 Oct 2017 15:36:20 +0200</pubDate>
        <link>http://localhost:4000/differential-privacy/</link>
        <guid isPermaLink="true">http://localhost:4000/differential-privacy/</guid>
        
        
        <category>differential-privacy</category>
        
      </item>
    
      <item>
        <title>Bloom Filters</title>
        <description>&lt;!-- http://jsbin.com/bocovofixo/edit?html,css,js,output --&gt;

&lt;p&gt;Probabilistic data structures are great. They allow us to be more efficient in
terms of time or space at the cost of only returning an approximate result.
Bloom filters are a popular such data structure. When I recently learned more
about their use cases, I found Bloom filters to be quite fascinating, so they
seem like a good topic to write a blog post about.&lt;/p&gt;

&lt;p&gt;In a nutshell, Bloom filters allow testing for set membership in a highly
efficient way. The trade-off is that they only return an approximate result.
Bloom filters either tell us that a value is definitely not in the set or that
it is &lt;em&gt;probably&lt;/em&gt; in the set.
If we consider being in the set as a positive result, this means they allow for
false positives but not for false negatives.&lt;/p&gt;

&lt;p&gt;The remainder of this blog post gives a more thorough introduction to Bloom filters and their
applications. Generally, the only prerequisites for being able to follow this
introduction are having a basic understanding of hashing and knowing what a set
is. To illustrate the ideas on a more intuitive level, I also coded up some live
demos and embedded them here.&lt;/p&gt;

&lt;h3 id=&quot;general-structure&quot;&gt;General structure&lt;/h3&gt;

&lt;p&gt;The underlying data structure for a Bloom filter is a bit array. Individual bits
in this bit array can either be set or not set. These possible states correspond
to values of 1 and 0. To efficiently change the values of bits, bitwise
operations can be used. As these are quite straight-forward, we will not go into
more detail here and just assume that we have operations for reading and writing
individual bits available.&lt;/p&gt;

&lt;p&gt;To get started with Bloom filters, let’s first consider a simple version of
them. To insert a value into the set, we use a hash function to map the value to
a valid index for the bit array. Then we change the bit corresponding to that
index to 1.&lt;/p&gt;

&lt;p&gt;To test if a value is part of our set, we hash the value and read
the corresponding bit. If it is 0, the value is definitely not in the set,
otherwise that bit would’ve been changed to 1. If the bit is 1, this
tells us that the value is &lt;em&gt;probably&lt;/em&gt; in the set. Probably, because there is a
chance that another element with the same hash value was inserted. In other
words, hash collisions are possible.&lt;/p&gt;

&lt;h3 id=&quot;evaluating-the-simple-solution&quot;&gt;Evaluating the simple solution&lt;/h3&gt;

&lt;p&gt;Below, you can find a live demo that can be used to test how well this works. As
you will notice, if the bit array is large and we insert relatively few
elements, then the probability of a hash collision is quite low. Of course, we
are assuming that the hash function is approximately uniformly distributed. As
we insert more values, hash collisions get more common and, at some point, we will
get too many false positives. Note that the Bloom filter below has only 32 bits
for visualization purposes.
This is an extraordinarily small Bloom filter, so we can’t add many elements
while still maintaining an acceptable false positive rate.&lt;/p&gt;

&lt;div id=&quot;bloom-simple&quot;&gt;&lt;/div&gt;

&lt;p&gt;This already highlights one important property of Bloom filters: We should have
some estimate for how many values we want to insert. This estimate should then
be used to choose an appropriate size for the bit array. By choosing a large
enough size, we can ensure that the expected number of false positives is quite
low.&lt;/p&gt;

&lt;p&gt;Let’s take a moment to reflect on this solution. Under the assumption that we
choose the array size to be large enough and that some false positives are
acceptable, we have a very fast way of testing for set membership.
Conceptionally, this solution is also pretty simple. We can think of it as a
HashSet that ignores the possibility of collisions. Because we only store a bit
array, Bloom filters are also quite space-efficient, especially when compared to
methods that store the original values, like HashSets.&lt;/p&gt;

&lt;h3 id=&quot;using-multiple-hash-functions&quot;&gt;Using multiple hash functions&lt;/h3&gt;

&lt;p&gt;In terms of efficiency, the data structure given above is pretty much perfect.
The two important operations, inserting and testing membership, are both
performed in constant time. What we primarily want to improve now is the
probability of false positives.&lt;/p&gt;

&lt;p&gt;This is where Bloom filters start to get interesting. Instead of just using a
single hash function, we use &lt;em&gt;k&lt;/em&gt; hash functions. So for each value we insert, we
get &lt;em&gt;k&lt;/em&gt; indices where we set the bits to 1. For testing membership, we check
the bits corresponding to &lt;em&gt;k&lt;/em&gt; indices.&lt;/p&gt;

&lt;div id=&quot;bloom-normal&quot;&gt;&lt;/div&gt;

&lt;p&gt;This significantly decreases the chance of getting false positives. Given a
large Bloom filter without many entries, it’s unlikely to get one hash
collision. But getting &lt;em&gt;k&lt;/em&gt; collisions at the same time is even more unlikely if
most bits are not set.&lt;/p&gt;

&lt;h3 id=&quot;deciding-on-the-number-of-hash-functions&quot;&gt;Deciding on the number of hash functions&lt;/h3&gt;

&lt;p&gt;More hash functions only help until a certain point. As an extreme example,
using as many hash functions as bits would make a Bloom filter totally
useless. On a similar note, when only having 32 bits available, using 3 hash
functions fills up the bit array too quickly, as you might have noticed in the
live demo above. It turns out that the optimal number of hash functions depends
on the bit array size and on how many elements we expect to be added.&lt;/p&gt;

&lt;p&gt;For &lt;em&gt;n&lt;/em&gt; added elements and a bit array size of &lt;em&gt;m&lt;/em&gt;, the optimal number of hash
functions &lt;em&gt;k&lt;/em&gt; is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;k = \frac{m}{n} * \ln(2)&lt;/script&gt;

&lt;p&gt;On a first look, this formula seems a bit cryptic. The logarithm is due to the
fact that we’re estimating the probability of false positives using &lt;a href=&quot;https://en.wikipedia.org/wiki/Azuma%27s_inequality&quot;&gt;Azuma’s
inequality&lt;/a&gt;, which uses the
exponential function. By transforming that inequality, we end up with the natural
logarithm. Other than that, the formula is easy to interpret. As the
bit array size increases in comparison to the expected number of added elements,
the optimal number of hash functions increases linearly.&lt;/p&gt;

&lt;p&gt;Deciding on the bit array size is also pretty straight-forward. A larger bit
array size always decreases the false positive rate. The downside is that more
space is needed. Generally, this is a trade-off where we choose the exact value
depending on the use case.&lt;/p&gt;

&lt;h3 id=&quot;applications&quot;&gt;Applications&lt;/h3&gt;

&lt;p&gt;After having only talked about the technical parts so far, let’s take a step
back and look at some applications.
Generally, Bloom filters are useful when a few false positives are acceptable to
be more space and time efficient, but false negatives are not.
In the next two subsections, we’ll go into more detail for two prime examples for Bloom filters.&lt;/p&gt;

&lt;h4 id=&quot;spelling-correction&quot;&gt;Spelling correction&lt;/h4&gt;

&lt;p&gt;To implement spelling correction, we need some way to decide whether a word is
misspelled. The Oxford English Dictionary contains more than 200,000 words.
Having all these words stored in memory all the time is a bad idea.&lt;/p&gt;

&lt;p&gt;Instead, we can insert all words from the dictionary into a Bloom filter to be
much more space efficient. The fact that false positives are possible means that
there will be a few misspelled words that will not be detected. However, no
correctly written words would be marked as incorrect. This is good because it
would only annoy users.&lt;/p&gt;

&lt;h4 id=&quot;databases&quot;&gt;Databases&lt;/h4&gt;

&lt;p&gt;Querying a database can be expensive, especially when it requires IO operations.
&lt;a href=&quot;http://cassandra.apache.org&quot;&gt;Cassandra&lt;/a&gt; uses Bloom filters to make &lt;a href=&quot;http://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutReads.html&quot;&gt;reading data&lt;/a&gt;
more efficient. It’s a first filter that checks if it’s possible that a key is
contained in some table. This allows Cassandra to prevent many expensive memory
calls.&lt;/p&gt;

&lt;p&gt;A few false positives are not a problem here. They just lead to performing the
normal expensive call that would be performed anyways if the Bloom filter would
not be used at all.&lt;/p&gt;

&lt;h3 id=&quot;other-set-operations&quot;&gt;Other set operations&lt;/h3&gt;

&lt;p&gt;So far we only discussed two operations, adding elements and testing for
membership. Depending on the application, other operations are also interesting.
In the following, we will focus on the classical set operations, union and
intersection. Afterwards, we’ll also look at removing elements.&lt;/p&gt;

&lt;p&gt;In the next two subsections, we assume that all Bloom filters use the same hash
functions and have the same number of bits.&lt;/p&gt;

&lt;h4 id=&quot;union&quot;&gt;Union&lt;/h4&gt;

&lt;p&gt;Union is straight-forward to implement for Bloom filters. We simply
create a Bloom filter where a bit is set when it’s also set in any input Bloom
filter. This resulting Bloom filter behaves exactly as when we directly query
all original Bloom filters and only returning true if at least one individual Bloom
filters returned true.&lt;/p&gt;

&lt;h4 id=&quot;intersection&quot;&gt;Intersection&lt;/h4&gt;

&lt;p&gt;For implementing an intersection operation, we can try to follow the same idea:
Construct a Bloom filter where a bit is set when all one input Bloom filter
had the bit set. It turns out that this is not a perfect solution because it
will lead to more false positives compared to directly querying the individual
Bloom filters.&lt;/p&gt;

&lt;p&gt;To understand why, it helps to think of a Bloom filter with two hash functions.
A value is part of this Bloom filter if its respective two bits are set. In the
Bloom filter resulting from the intersection it is possible that these bits were
set because of several different values that are not in the intersection themselves.
This would not happen if we build a new Bloom filter directly from the set
intersection.&lt;/p&gt;

&lt;h3 id=&quot;removing-values&quot;&gt;Removing values&lt;/h3&gt;

&lt;p&gt;Removing values from standard Bloom filters is difficult. By just setting the
corresponding bits to 0, we could accidentally introduce false negatives. This is
due to the fact that a bit maybe also needs to be set for a different added
element. We generally want to avoid false negatives with Bloom filters, so this
is not an acceptable solution.&lt;/p&gt;

&lt;p&gt;One possible solution is introducing a second Bloom filter that keeps track of
the removed values. This only works if values cannot be readded. Still, this is
not a satisfying solution since false positives in this second Bloom filter
become false negatives in the first Bloom filter.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Counting Bloom Filters&lt;/em&gt; are a more sophisticated alternative. Instead of just
using bits as boolean indicators, enough bits to keep a count at each index are
used. Then, instead of setting a bit, the count at the respective position is
increased by 1. For removing an element, the counter is decreased by 1. This
works well as long as an element is not added more than once before removing it.&lt;/p&gt;

&lt;div id=&quot;bloom-counting&quot;&gt;&lt;/div&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;p&gt;Bloom Filters are a probabilistic data structure that allow for testing set
membership in an extremely efficient way. Since they are based on bitwise operations,
they also require very little space. The trade-off is that there is a small
probability of false positives.
These false positives can be reduced by using enough bits and multiple hash functions.
There are many interesting use-cases for Bloom Filters, for example to make
caching in databases more efficient.&lt;/p&gt;

&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://fb.me/react-15.1.0.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://fb.me/react-dom-15.1.0.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/crypto-js/3.1.2/components/core-min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/crypto-js/3.1.2/rollups/hmac-md5.js&quot;&gt;&lt;/script&gt;

&lt;style&gt;
.bloom-filter {
  width: 581px;
  position: relative;
  margin-left: 30px;
  margin-bottom: 15px;
}

.bloom-filter h2 {
  font-size: 17px;
  display: inline-block;
  margin: 0;
}

.bloom-filter ul {
  list-style-type: none;
  margin: 10px auto;
  padding: 0;
}

.bloom-filter li {
  display: inline-block;
  width: 17px;
  height: 17px;
  padding: 0;
  text-indent: 0;
  border: 1px solid #565656;
  border-right: none;
  text-align: center;
  font-size: 13px;
  vertical-align:top
}

.bloom-filter li:before {
  content: '';
  padding: 0;
}


.bloom-filter ul :last-child {
  border-right: 1px solid;
}

.bloom-filter .set {
  background: grey;
  transition: background .5s ease-in;
}

.bloom-filter form {
  padding-bottom: 10px;
}

.bloom-filter input {
  display: inline-block;
  position: relative;
  vertical-align: top;
}

.bloom-filter input[type=&quot;text&quot;] {
  width: 150px;
  height: 15px;
  padding: 4px 6px;
  font-size: 14px;
  float: none;
  margin-left: 0;
  background-color: #ffffff;
  border: 1px solid #cccccc;
  outline: none;
  line-height: 20px;
  color: #555555;
  font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif;
  border-radius: 4px 0 0 4px;
}

.bloom-filter input[type=&quot;submit&quot;], .bloom-filter input[type=&quot;button&quot;] {
  min-width: 100px;
  height: 25px;
  line-height: 15px;
  margin-left: -3px;
  padding: 4px 12px;
  font-size: 14px;
  color: #333333;
  text-align: center;
  text-shadow: 0 1px 1px rgba(255, 255, 255, 0.75);
  cursor: pointer;
  background-color: #e6e6e6;
  background-image: -moz-linear-gradient(top, #ffffff, #e6e6e6);
  background-image: -webkit-gradient(linear, 0 0, 0 100%, from(#ffffff), to(#e6e6e6));
  background-image: -webkit-linear-gradient(top, #ffffff, #e6e6e6);
  background-image: -o-linear-gradient(top, #ffffff, #e6e6e6);
  background-image: linear-gradient(to bottom, #ffffff, #e6e6e6);
  background-repeat: repeat-x;
  border: 1px solid #cccccc;*
  border: 0;
  border-color: #e6e6e6 #e6e6e6 #bfbfbf;
  border-color: rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.1) rgba(0, 0, 0, 0.25);
  border-bottom-color: #b3b3b3;
  font-family: &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif;
}

.bloom-filter input[type=&quot;submit&quot;]:active, .bloom-filter input[type=&quot;button&quot;]:active {
  background-color: #ffffff;
  background-image: -moz-linear-gradient(bottom, #ffffff, #e6e6e6);
  background-image: -webkit-gradient(linear, 0 0, 0 100%, from(#e6e6e6), to(#ffffff));
  background-image: -webkit-linear-gradient(bottom, #ffffff, #e6e6e6);
  background-image: -o-linear-gradient(bottom, #ffffff, #e6e6e6);
  background-image: linear-gradient(to top, #ffffff, #e6e6e6);
  background-repeat: repeat-x;
}

.bloom-filter .last-input {
  border-radius: 0 4px 4px 0;
}

.bloom-filter .elements {
  display: inline-block;
}
&lt;/style&gt;

&lt;script&gt;
&quot;use strict&quot;;

var _createClass = function () { function defineProperties(target, props) { for (var i = 0; i &lt; props.length; i++) { var descriptor = props[i]; descriptor.enumerable = descriptor.enumerable || false; descriptor.configurable = true; if (&quot;value&quot; in descriptor) descriptor.writable = true; Object.defineProperty(target, descriptor.key, descriptor); } } return function (Constructor, protoProps, staticProps) { if (protoProps) defineProperties(Constructor.prototype, protoProps); if (staticProps) defineProperties(Constructor, staticProps); return Constructor; }; }();

function _possibleConstructorReturn(self, call) { if (!self) { throw new ReferenceError(&quot;this hasn't been initialised - super() hasn't been called&quot;); } return call &amp;&amp; (typeof call === &quot;object&quot; || typeof call === &quot;function&quot;) ? call : self; }

function _inherits(subClass, superClass) { if (typeof superClass !== &quot;function&quot; &amp;&amp; superClass !== null) { throw new TypeError(&quot;Super expression must either be null or a function, not &quot; + typeof superClass); } subClass.prototype = Object.create(superClass &amp;&amp; superClass.prototype, { constructor: { value: subClass, enumerable: false, writable: true, configurable: true } }); if (superClass) Object.setPrototypeOf ? Object.setPrototypeOf(subClass, superClass) : subClass.__proto__ = superClass; }

function _classCallCheck(instance, Constructor) { if (!(instance instanceof Constructor)) { throw new TypeError(&quot;Cannot call a class as a function&quot;); } }

var BloomFilter = function () {
  function BloomFilter(num_bits, num_hash_functions) {
    _classCallCheck(this, BloomFilter);

    this.num_bits = num_bits;
    this.num_hash_functions = num_hash_functions;
    this._init_storage();
  }

  _createClass(BloomFilter, [{
    key: &quot;_init_storage&quot;,
    value: function _init_storage() {
      this.storage = Array(this.num_bits);
      for (var i = 0; i &lt; this.num_bits; i++) {
        this.storage[i] = false;
      }
    }
  }, {
    key: &quot;hash&quot;,
    value: function hash(value) {
      var seed = arguments.length &gt; 1 &amp;&amp; arguments[1] !== undefined ? arguments[1] : 0;

      return Math.abs(CryptoJS.MD5(value + seed).words.reduce(function (a, b) {
        return a + b;
      }), 0) % this.num_bits;
    }
  }, {
    key: &quot;add&quot;,
    value: function add(value) {
      for (var i = 0; i &lt; this.num_hash_functions; i++) {
        var hashed = this.hash(value, i);
        this.storage[hashed] = true;
      }
    }
  }, {
    key: &quot;contains&quot;,
    value: function contains(value) {
      for (var i = 0; i &lt; this.num_hash_functions; i++) {
        var hashed = this.hash(value, i);
        if (!this.storage[hashed]) return false;
      }

      return true;
    }
  }, {
    key: &quot;print&quot;,
    value: function print() {
      return this.storage.reduce(function (result, bit) {
        if (bit) {
          return result + &quot;x&quot;;
        } else {
          return result + &quot;_&quot;;
        }
      }, &quot;&quot;);
    }
  }]);

  return BloomFilter;
}();

var CountingBloomFilter = function (_BloomFilter) {
  _inherits(CountingBloomFilter, _BloomFilter);

  function CountingBloomFilter() {
    _classCallCheck(this, CountingBloomFilter);

    return _possibleConstructorReturn(this, (CountingBloomFilter.__proto__ || Object.getPrototypeOf(CountingBloomFilter)).apply(this, arguments));
  }

  _createClass(CountingBloomFilter, [{
    key: &quot;_init_storage&quot;,
    value: function _init_storage() {
      this.storage = Array(this.num_bits);
      for (var i = 0; i &lt; this.num_bits; i++) {
        this.storage[i] = 0;
      }
    }
  }, {
    key: &quot;add&quot;,
    value: function add(value) {
      for (var i = 0; i &lt; this.num_hash_functions; i++) {
        var hashed = this.hash(value, i);
        this.storage[hashed] += 1;
      }
    }
  }, {
    key: &quot;print&quot;,
    value: function print() {
      return this.storage.reduce(function (result, bit) {
        if (bit) {
          return result + String(bit);
        } else {
          return result + &quot;_&quot;;
        }
      }, &quot;&quot;);
    }
  }, {
    key: &quot;remove&quot;,
    value: function remove(value) {
      for (var i = 0; i &lt; this.num_hash_functions; i++) {
        var hashed = this.hash(value, i);
        this.storage[hashed] -= 1;
      }
    }
  }]);

  return CountingBloomFilter;
}(BloomFilter);

function plural(base, extension, n) {
  return n + &quot; &quot; + base + (n == 1 ? &quot;&quot; : extension);
}

var BloomFilterVisualization = function (_React$Component) {
  _inherits(BloomFilterVisualization, _React$Component);

  function BloomFilterVisualization(props) {
    _classCallCheck(this, BloomFilterVisualization);

    var _this2 = _possibleConstructorReturn(this, (BloomFilterVisualization.__proto__ || Object.getPrototypeOf(BloomFilterVisualization)).call(this, props));

    _this2.state = {
      bf: props.counting ? new CountingBloomFilter(props.bits, props.hash_functions) : new BloomFilter(props.bits, props.hash_functions),
      bits: props.bits,
      hash_functions: props.hash_functions,
      addedValues: [],
      simple: props.simple || false,
      lastCheck: &quot;&quot;,
      counting: props.counting || false
    };
    return _this2;
  }

  _createClass(BloomFilterVisualization, [{
    key: &quot;render&quot;,
    value: function render() {
      var data_structure = &quot;Bloom Filter&quot;;
      if (this.state.counting) {
        data_structure = &quot;Counting &quot; + data_structure;
      }

      return React.createElement(
        &quot;div&quot;,
        { className: &quot;bloom-filter&quot; },
        React.createElement(
          &quot;h2&quot;,
          null,
          &quot;Live Demo: &quot;,
          data_structure,
          &quot; with &quot;,
          this.state.bits,
          &quot; bits and &quot;,
          plural(&quot;hash function&quot;, &quot;s&quot;, this.state.hash_functions),
          &quot; &quot;
        ),
        React.createElement(
          &quot;ul&quot;,
          null,
          this.state.bf.storage.map(this.renderBit.bind(this))
        ),
        this.renderForm(),
        React.createElement(
          &quot;div&quot;,
          { className: &quot;elements&quot; },
          &quot;Added so far: &quot;,
          &quot;{&quot;,
          &quot; &quot;,
          this.state.addedValues.join(&quot;, &quot;),
          &quot; &quot;,
          &quot;}&quot;
        ),
        this.state.lastCheck != &quot;&quot; ? React.createElement(
          &quot;div&quot;,
          null,
          this.state.lastCheck
        ) : &quot;&quot;
      );
    }
  }, {
    key: &quot;renderBit&quot;,
    value: function renderBit(bit, i) {
      var className = &quot;&quot;,
          content = &quot;&quot;;

      if (this.state.counting) {
        content = bit == 0 ? &quot;&quot; : String(bit);
      } else {
        className = bit ? &quot;set&quot; : &quot;not-set&quot;;
      }

      return React.createElement(
        &quot;li&quot;,
        { className: className, key: i },
        content
      );
    }
  }, {
    key: &quot;renderForm&quot;,
    value: function renderForm() {
      return React.createElement(
        &quot;form&quot;,
        { onSubmit: this.add.bind(this) },
        React.createElement(&quot;input&quot;, { type: &quot;text&quot;, ref: &quot;value&quot; }),
        this.state.simple ? React.createElement(&quot;input&quot;, { type: &quot;submit&quot;, value: &quot;Add&quot;, className: &quot;last-input&quot; }) : &quot;&quot;,
        !this.state.simple ? React.createElement(&quot;input&quot;, { type: &quot;submit&quot;, value: &quot;Add&quot; }) : &quot;&quot;,
        this.state.counting ? React.createElement(&quot;input&quot;, { type: &quot;button&quot;, value: &quot;Remove&quot;, onClick: this.remove.bind(this) }) : &quot;&quot;,
        !this.state.simple ? React.createElement(&quot;input&quot;, { type: &quot;button&quot;, value: &quot;Check for membership&quot;, className: &quot;last-input&quot;, onClick: this.check.bind(this) }) : &quot;&quot;
      );
    }
  }, {
    key: &quot;add&quot;,
    value: function add(e) {
      e.preventDefault();

      var input = this.refs.value;
      var value = input.value;
      input.select();

      if (value.trim() == &quot;&quot;) return false;

      var addedValues = this.state.addedValues;
      var inSet = addedValues.indexOf(value) != -1;

      if (this.state.counting &amp;&amp; inSet) {
        var confirmed = confirm(&quot;Adding an already added value will partly break the Counting Bloom filter. Do you still want to continue?&quot;);
        if (!confirmed) {
          return false;
        }
      }

      var bf = this.state.bf;
      bf.add(value);

      if (!inSet) addedValues.push(value);

      this.setState({
        bf: bf,
        addedValues: addedValues
      });
    }
  }, {
    key: &quot;check&quot;,
    value: function check(e) {
      e.preventDefault();

      var input = this.refs.value;
      var value = input.value;
      input.select();

      if (value.trim() == &quot;&quot;) return false;

      var bf = this.state.bf;
      var isContained = bf.contains(value);

      this.setState({
        lastCheck: value + &quot; was &quot; + (isContained ? &quot;&quot; : &quot;not &quot;) + &quot;found&quot;
      });
    }
  }, {
    key: &quot;remove&quot;,
    value: function remove(e) {
      e.preventDefault();

      var input = this.refs.value;
      var value = input.value;
      input.select();

      if (value.trim() == &quot;&quot;) return false;

      var addedValues = this.state.addedValues;
      var inSet = addedValues.indexOf(value) != -1;

      if (!inSet) {
        var confirmed = confirm(&quot;Removing a value that's not in the set will partly break the Counting Bloom filter. Do you still want to continue?&quot;);
        if (!confirmed) {
          return false;
        }
      }

      this.state.bf.remove(value);
      addedValues.splice(addedValues.indexOf(value), 1);

      this.setState({
        addedValues: addedValues
      });
    }
  }]);

  return BloomFilterVisualization;
}(React.Component);

ReactDOM.render(React.createElement(BloomFilterVisualization, { bits: 32, hash_functions: 1 }), document.getElementById(&quot;bloom-simple&quot;));
ReactDOM.render(React.createElement(BloomFilterVisualization, { bits: 32, hash_functions: 3 }), document.getElementById(&quot;bloom-normal&quot;));
ReactDOM.render(React.createElement(BloomFilterVisualization, { bits: 32, hash_functions: 3, counting: true }), document.getElementById(&quot;bloom-counting&quot;));
&lt;/script&gt;

</description>
        <pubDate>Sun, 03 Sep 2017 21:36:20 +0200</pubDate>
        <link>http://localhost:4000/bloom-filters/</link>
        <guid isPermaLink="true">http://localhost:4000/bloom-filters/</guid>
        
        
        <category>probabilistic-data-structures</category>
        
      </item>
    
  </channel>
</rss>

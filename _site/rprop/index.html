<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width initial-scale=1">

  <title>RProp</title>
  <meta name="description" content="RProp is a popular gradient descent algorithm that only uses the signs ofgradients to compute updates [1] [2]. It stands for Resilient Propagation and worksw...">

  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/rprop/">
  <link rel="alternate" type="application/atom+xml" title="Florian Hartmann" href="http://localhost:4000/feed.xml" />
</head>

  <body>
    <div class="wrapper">
      <div class="page-content">
        <div class="post">
  <header class="post-header">
      <a href=" /  " class="home-link">← Home</a>
    <h1 class="post-title">RProp</h1>
    <p class="post-meta">April 8, 2018</p>
  </header>

  <article class="post-content">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><em>RProp</em> is a popular gradient descent algorithm that only uses the signs of
gradients to compute updates <a href="#citation-1" id="ref-1" class="ref-link">[1]</a>
 <a href="#citation-2" id="ref-2" class="ref-link">[2]</a>
. It stands for <em>Resilient Propagation</em> and works
well in many situations because it adapts the step size dynamically for each
weight independently. This blog posts gives an introduction to RProp
and motivates its design choice of ignoring gradient magnitudes.</p>

<p>Most gradient descent variants use the sign and the magnitude of the gradient.
The gradient points in the direction of steepest ascent.
Because we typically want to find a minimum, we follow the gradient in the
opposite direction.
This direction is completely determined by the sign of the gradient.</p>

<h3 id="gradient-magnitudes">Gradient magnitudes</h3>

<p>To decide on the step size, a scaled version of the gradient’s magnitude is
generally used by most gradient descent algorithms.
This heuristic often works well but there is no guarantee that it is
always a good choice.
To see that it can work extremely badly, and does not have
to contain valuable information, we consider a function \(f\).
The plots below show \(f\) as well as two scaled versions.</p>

<table class="image">
	<caption align="bottom" style="">Three functions with the same optima but vastly different gradients</caption>
	<tr><td><img src="/assets/posts/rprop/scales.png" alt="Three functions with the same optima but vastly different gradients" width="" /></td></tr>
</table>

<p>All three of these functions have the exact same optima, so the step updates
using gradient descent should all be similar.
However, if we determine the step size using the gradient’s
magnitude, then the step sizes for the three functions differ by
orders of magnitude.
Even worse, the gradient virtually vanishes for the second function and explodes
for the third, as shown below:</p>

<table class="image">
	<caption align="bottom" style="">The first derivatives of the three functions </caption>
	<tr><td><img src="/assets/posts/rprop/scales-diff.png" alt="The first derivatives of the three functions " width="" /></td></tr>
</table>

<p>This shows that the gradient’s magnitude does not necessarily contain useful
information for determining the step size.
Even though optima can still be found by choosing appropriate learning rates,
this makes it clear that using the gradient’s magnitude at all is sometimes questionable.
Using a fixed learning rate will also fail if only some parts of the function
are scaled.</p>

<h3 id="updating-weights">Updating weights</h3>

<p>Modern gradient descent variants try to circumvent this problem by dynamically
adapting the step size.
RProp does this in a way that only requires the sign of the gradient.
By ignoring the gradient’s magnitude, RProp has no problems if a function has a few very
steep areas.</p>

<p>Concretely, RProp uses a different step size for each dimension.
Let \(\eta_i^{(t)}\) be the step size for the \(i\)-th weight in the \(t\)-th
iteration of gradient descent.
The value for the first and second iteration, \(\eta_i^{(0)}\) and
\(\eta_i^{(1)}\), is a hyperparameter that needs to be chosen in advance.
This step size is then dynamically adapted for each weight, depending on the gradient.</p>

<p>The weights themselves are updated using</p>

<script type="math/tex; mode=display">w_i^{(t)} = w_i^{(t - 1)} - \eta_i^{(t - 1)} * \operatorname{sgn}\left(\frac{\partial E^{(t -
    1)}}{\partial w_i^{(t - 1)}}\right)</script>

<p>where the sign of the partial derivative of the error in the last step
with respect to the respective weight is computed.
We go in the direction of descent using the determined step size.</p>

<h3 id="adapting-the-step-size">Adapting the step size</h3>

<p>In each iteration of RProp, the gradients are computed and the step sizes are
updated for each dimension individually.
This is done by comparing the gradient’s sign of the current and previous
iteration.
The idea here is the following:</p>

<ul>
  <li>When the signs are the same, we go in the same direction as in the
  previous iteration. Since this seems to be a good direction, the step size
  should be increased to go to the optimum more quickly</li>
  <li>If the sign changed, the new update is moving in a different direction.
  This means that we just jumped over an optimum.
  The step size should be decreased to avoid jumping over the optimum again</li>
</ul>

<p>A visualization of this idea is shown below.</p>

<table class="image">
	<caption align="bottom" style="">The gradient direction changes when jumping over optima</caption>
	<tr><td><img src="/assets/posts/rprop/jumps.png" alt="The gradient direction changes when jumping over optima" width="" /></td></tr>
</table>

<p>To implement this update scheme, the following formula is used:</p>

<script type="math/tex; mode=display">% <![CDATA[
\eta_i^{(t)} = \begin{cases}
    \min(\eta_i^{(t - 1)} * \alpha, \eta_{\max}) & \text{if } \frac{\partial E^{(t)}}{\partial w_i^{(t)}} * \frac{\partial E^{(t - 1)}}{\partial w_i^{(t - 1)}} > 0 \\
    \max(\eta_i^{(t - 1)} * \beta, \eta_{\min}) & \text{if } \frac{\partial E^{(t)}}{\partial w_i^{(t)}} * \frac{\partial E^{(t - 1)}}{\partial w_i^{(t - 1)}} < 0 \\
    \eta_i^{(t - 1)} & \text{otherwise}
    \end{cases}
\label{eq:rprop} %]]></script>

<p>where \(\alpha &gt; 1 &gt; \beta\) scale the step size, depending on whether
the speed should be increased or decreased. The step size is then clipped using
\(\eta_{\min}\) and \(\eta_{\max}\) to avoid it becoming too large or too small.
If a gradient was zero, a local optimum for this weight was found and the step
size is not changed.</p>

<h3 id="hyperparameters">Hyperparameters</h3>

<p>These seem like many hyperparameters to choose, but in practice there are known values for them that generally work well.
It is also not problematic if the clipping values \(\eta_{\min}\) and \(\eta_{\max}\) are respectively smaller and larger than necessary because an inconvenient step size is generally adapted quickly.</p>

<p>Popular values for \(\alpha\) and \(\beta\) are \(1.2\) and \(0.5\).
Heuristically, it works well to increase the step size slowly, while allowing for the possibility of quickly decreasing it when jumping around an optimum.
For fine-tuning the weights, it is important that \(\beta\) is not the reciprocal of \(\alpha\), to allow for many different step sizes.</p>

<h3 id="conclusion">Conclusion</h3>

<p>One advantage of RProp that was not discussed so far is having a different step
size for each weight.
If one weight is already very close to its optimal value while a second weight
still needs to be changed a lot, this is not a problem for RProp.
Other gradient descent variants can have much more problems with such a
situation, especially because the gradient magnitudes can be misleading here.</p>

<p>While RProp works well in a lot of situations, it is not perfect.
For instance, RProp generally requires large batch updates.
If there’s too much randomness in stochastic gradient descent, then the step sizes jump around too much
and the updates work badly.</p>

<p>Implementing RProp is quite straightforward.
To get a better understanding of RProp, reading the <a href="https://github.com/pytorch/pytorch/blob/master/torch/optim/rprop.py">PyTorch
implementation</a> can also be helpful.</p>

<h3 id="references">References</h3>

<ol class="references-list">
	
	<li><span id="citation-1">Rojas, R., 2013. Neural networks: a systematic introduction. Springer Science &amp; Business Media. <a href="#ref-1" class="ref-backlink"></a></span></li>

	<li><span id="citation-2">Riedmiller, M. and Braun, H., 1993. A direct adaptive method for faster backpropagation learning: The RPROP algorithm. In Neural Networks, 1993., IEEE International Conference on (pp. 586-591). IEEE. <a href="#ref-2" class="ref-backlink"></a></span></li>


</ol>


  </article>

  
  <hr>

  <div class="related">
    <h2>Other Posts</h2>
    
      <li><a href="/federated-learning-firefox/" title="Federated Learning for Firefox">Federated Learning for Firefox
       &nbsp; <span class="post-meta">August 27, 2018</span></a>
    
      <li><a href="/estimators/" title="Estimation Theory and Machine Learning">Estimation Theory and Machine Learning
       &nbsp; <span class="post-meta">July 17, 2018</span></a>
    
      <li><a href="/federated-learning/" title="Federated Learning">Federated Learning
       &nbsp; <span class="post-meta">May 09, 2018</span></a>
    
  </div>
  
</div>

<script>
var anchorForId = function (id) {
  var anchor = document.createElement("a");
  anchor.className = "header-link";
  anchor.href      = "#" + id;
  anchor.innerHTML = "<i class=\"fas fa-link\"></i>";
  return anchor;
};

var linkifyAnchors = function (level, containingElement) {
  var headers = containingElement.getElementsByTagName("h" + level);
  for (var h = 0; h < headers.length; h++) {
    var header = headers[h];

    if (typeof header.id !== "undefined" && header.id !== "") {
      header.appendChild(anchorForId(header.id));
    }
  }
};

document.onreadystatechange = function () {
  if (this.readyState === "complete") {
    linkifyAnchors(3, document.body);
  }
};
</script>

<script>
var alreadySeenRefs = {};

document.querySelectorAll(".ref-link").forEach(function (a) {
  var id = a.getAttribute("id");

  if (id in alreadySeenRefs) return;

  var p = a.closest("p");
  var currentId = p.getAttribute("id");

  if (currentId == null) {
    p.setAttribute("id", id)
  } else {
    document.querySelector("#" + id.replace("ref", "citation") + " a.ref-backlink").setAttribute("href", "#" + currentId)
  }

  alreadySeenRefs[id] = true;
  a.removeAttribute("id")
})
</script>

      </div>
    </div>
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-124645768-1"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-124645768-1');
</script>

  </body>
</html>
